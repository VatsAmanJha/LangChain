{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG: Retrieval Augmented Generation\n",
    "\n",
    "- Phase 1\n",
    "    1. LOADING or Ingestion a data(website,book)\n",
    "    2. SPLITTING a data(text chunk)\n",
    "    3. EMBEDDING convect text into vector\n",
    "    4. Storing vector data in vector db\n",
    "\n",
    "- Phase 2\n",
    "    1. Question ask by user\n",
    "    2. Retrieve the relevant chunk \n",
    "    3. Prompting\n",
    "    4. Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader,PyPDFLoader,JSONLoader,WebBaseLoader,ArxivLoader,WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=TextLoader(\"speech.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=PyPDFLoader(\"speech.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x16f2a1280d0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.pdf', 'page': 0}, page_content='Prime Minister Narendra Modi\\'s recent speeches emphasize India\\'s future ambitions and developmental \\nstrategies. For instance, during the 78th Independence Day address, Modi focused on India becoming a \\nglobal leader in several sectors, including renewable energy and high-tech manufacturing. He highlighted \\nprojects like the \"Green Hydrogen Mission,\" which aims to make India a powerhouse in green energy, and\\n\"Design in India, Design for the World,\" encouraging domestic innovations with international impact. \\nAdditionally, his speech included goals to strengthen India\\'s position in semiconductor production, \\nenhance skill training, and make significant advances in public health through initiatives like the Swasth \\nBharat Mission   ')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=WebBaseLoader(\"https://pmc.ncbi.nlm.nih.gov/articles/PMC2083239/\",bs_kwargs=dict(parse_only=bs4.SoupStrainer(id=(\"article-container\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoupStrainer important attribute:\n",
    "1. Class_\n",
    "2. id\n",
    "3. name for tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x16f27b56150>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC2083239/'}, page_content=\"\\n\\n\\n\\n\\n\\n    As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\\n    the contents by NLM or the National Institutes of Health.\\n    Learn more:\\n    PMC Disclaimer\\n    |\\n    \\n        PMC Copyright Notice\\n    \\n\\n\\n\\n\\n\\n\\n\\nArch Dis Child. 2007 May 23;92(10):900–901. doi: 10.1136/adc.2007.118810\\n\\n\\nSearch in PMC\\nSearch in PubMed\\nView in NLM Catalog\\nAdd to search\\n\\nTreatment of fever and over‐the‐counter medicines\\nEdward Purssell\\nEdward Purssell\\nFind articles by Edward Purssell\\n\\n\\n\\n\\nAuthor information\\nArticle notes\\nCopyright and License information\\n\\n\\n\\n✉Correspondence to: Edward Purssell, King's College London, James Clerk Maxwell Building, 57 Waterloo Road, London SE1 8WA, UK;\\nedward.purssell@kcl.ac.uk\\n\\nAccepted 2007 May 13; Issue date 2007 Oct.\\nPMC Copyright notice\\n\\nPMCID: PMC2083239\\xa0\\xa0PMID: 17522164\\n\\nAbstract\\nHealth policy is aimed at increasing homecare and deregulating the supply of drugs. This study used parental reports of the treatment of fever as an indicator of possible problems that may result from this policy, finding that the use of ineffective treatments and the overuse of drugs were common.\\nKeywords: Child, preschool; body temperature; health policy; home care\\nFever is a common symptom of infection in children, and is often treated at home by parents and carers with the antipyretic drugs ibuprofen and paracetamol. Current government policy is aimed at increasing the scope of homecare and the diversity of care providers, and extending the use of over‐the‐counter medicines for the treatment of common disorders.1 Using the treatment of febrile children as a model, this study aimed to establish from whom parents received information about the homecare of febrile children, how useful they perceived that information to be and what their actual practices were.\\nMethods\\nData were collected using a questionnaire that was completed by a convenience sample of 181 parents attending an outpatients clinic at a London hospital. Questions concerned their general beliefs about fever and its harmful effects, as well as their treatment behaviours and sources of information. Because of limited translation facilities, only those who could read English were included. All those approached agreed to participate.Results\\nOf the 181 respondents, 101 (56%) were in the 31–40‐year‐old age group, with 44 (24%) being less than 20\\u2005years old. Nearly all had some form of formal qualification: 56 (31%) had a bachelors degree or higher and only six (3%) had no formal qualification. Most respondents were mothers of children, although 18 (10%) were fathers. All but 26 (14%) had more than one child.\\nThe first group of questions were about treatment of fever. Most parents used a thermometer to take their child's temperature, although 46 (25%) used touch. The most common treatments were paracetamol alone (used by 93, 51%), sponging (61, 34%), paracetamol and ibuprofen alternately (47, 26%) and together (28, 15%) and ibuprofen alone (17, 9%). When asked how often they administered antipyretic drugs, eight (4%) responded that they gave paracetamol every 2\\u2005h, and 63 (35%) gave ibuprofen every 4\\u2005h or more often.\\nWhen asked from whom they had learned their information about the treatment of fever, doctors were the most cited source, followed by friends and then books and magazines. The least used sources of information were the internet, nurses, pharmacists and NHS Direct (table 1). In response to a question about how useful they had found this information, all sources were thought to have been of use by over 80% of those who reported using a particular source, with doctors (97%), nurses (95%) and pharmacists (91%) scoring particularly highly. The least useful sources of information were thought to have been the internet (82%), health visitors (87%), NHS Direct (88%) and friends (89%).\\n\\nTable 1\\u2003Sources of information about the treatment of fever.\\n\\n\\n\\n\\n\\n\\n\\nSource\\nNumber\\nUseful as a source (%)\\n\\n\\n\\nDoctors\\n136\\n97\\n\\n\\nFriends\\n117\\n89\\n\\n\\nBooks and magazines\\n91\\n87\\n\\n\\nHealth visitors\\n74\\n87\\n\\n\\nPharmacists\\n66\\n91\\n\\n\\nNHS Direct\\n64\\n88\\n\\n\\nNurses\\n44\\n95\\n\\n\\nInternet\\n31\\n82\\n\\n\\n\\nOpen in a new tab\\n*Some parents reported use of more than one source.Discussion\\nA significant number of parents in this study reported the use of ineffective treatments, treatments for which there are a lack of safety data, or incorrect use of antipyretic drugs. Tepid sponging is ineffective, and may cause distress or shivering, but does not cause significant harm.2 However, other reported practices may be less benign. Combinations of paracetamol and ibuprofen are widely used by professionals despite a lack of data supporting their use;3 these findings suggest that a significant number of parents are also using such combinations. The unnecessary use of combinations of drugs may also increase the risk of error. This is of particular concern as a significant number of parents were giving antipyretic drugs more regularly than recommended.\\nMost parents reported receiving advice about the treatment of fever, and most found the advice that they were given useful. Medical professionals were the source of much of this information. Although it is not possible to be sure that these incorrect practices emanated from professionals, other studies have demonstrated that their knowledge and practices are often not evidence based.4\\nThese findings are significant because some of the actions reported by parents carry the risk of toxicity. Additionally, the widespread misuse of antipyretic drugs may have implications for the government policy of increased homecare and deregulation of medicine supply.1 The National Service Framework for Children, Young People and Maternity Services states that there should be clear, understandable and up‐to‐date information about medicines.5 However, despite fever being a common symptom, the longstanding availability of antipyretics as over‐the‐counter (general sales list) medicines, and their increasing availability in non‐pharmacy settings, this study suggests that many parents remain confused about their safe use.\\nBefore further widespread deregulation of paediatric drugs, attention should be paid to ensuring that strategies are put in place to ensure that parents understand their use. Healthcare professionals are likely to remain the most important source of information, although the makeup of the professions involved and the way that information is given may change.\\nWhile it is possible to exert regulatory control over the actions of professionals and much of the media, not all sources of information are amenable to this. Although not widely used by these parents, the internet is likely to become an increasingly important source of information. Currently, direct advertising of pharmacy or over‐the‐counter medicines to consumers is permitted, although advertising of prescription‐only medicines is not. The international and unregulated nature of much information on the internet, and the consequent globalisation of drug information and even supply may weaken such national regulation.\\nIt is not known from where these parents obtained their antipyretic drugs. However, the increasing availability of drugs from non‐pharmacy sources means a passive approach to the provision of information about prescribed drugs may no longer be sufficient. Doctors and other professionals will have to take a more proactive role in establishing what drugs children are taking and how they are being given, both for immediate educational purposes and to ensure that medical records are accurate. The fact that the parents in this study were accessing secondary hospital care meant that such opportunities had arisen or were about to arise. Parents completed the questionnaire prior to the consultation, so these findings do not reflect what occurred in the clinic.\\nAlthough antipyretics and other over‐the‐counter drugs are generally safe, serious sequelae can occur from their incorrect use. This study used a sample from one London hospital and so is not necessarily representative of other groups. However, it does suggest there remains confusion about the use of antipyretic drugs, even though the educational level of these parents was relatively high and they were in a hospital environment. Continuing research is needed to monitor drug usage by parents to inform educational and drug supply policies.Acknowledgements\\nProfessor Alison While is thanked for assistance with planning the study and Billie Coomber for assistance with data collection.Footnotes\\nCompeting interests: None.References\\n\\n\\n1.British Medical Association Over‐the‐counter medication. London: British Medical Association, 2005\\n\\n\\n2.Meremikwu M, Oyo‐Ita A. Physical methods for treating fever in children. Cochrane Database Syst Rev 2003(2)CD004264. [DOI] [PMC free article] [PubMed]\\n\\n3.Mayoral C E, Mariono R V M, Rosenfeld W.et al Alternating antipyretics: is this an alternative? Pediatrics 2000105(5)1009–1012. [DOI] [PubMed] [Google Scholar]\\n\\n4.Sarrell M, Cohen H A, Kahan E. Physicians', nurses', and parents' attitudes to and knowledge about fever in early childhood. Patient Educ Couns 20024661–65. [DOI] [PubMed] [Google Scholar]\\n\\n5.Department of Health National Service Framework for Children, Young People and Maternity Services, medicines management for children. London: Department of Health, 2004\\n\\n\\nArticles from Archives of Disease in Childhood are provided here courtesy of BMJ Publishing Group\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nACTIONS\\n\\n\\n\\nView on publisher site\\n\\n\\n\\n\\n\\n\\n\\nPDF (92.1\\xa0KB)\\n\\n\\n\\n\\n\\n\\n\\nCite\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCollections\\n\\n\\n\\n\\n\\n\\n\\nPermalink\\n\\n\\n\\nPERMALINK\\n\\n\\n\\n\\n\\n\\nCopy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRESOURCES\\n\\n\\n\\n                            Similar articles\\n                        \\n\\n\\n\\n\\n\\n                             Cited by other articles\\n                        \\n\\n\\n\\n\\n\\n                                 Links to NCBI Databases\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCite\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopy\\n\\n\\n\\n\\n\\n\\n\\nDownload .nbib\\n.nbib\\n\\n\\n\\n\\nFormat:\\n\\n\\n        AMA\\n      \\n\\n        APA\\n      \\n\\n        MLA\\n      \\n\\n        NLM\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdd to Collections\\n\\n\\n\\n\\n\\n\\n\\nCreate a new collection\\n\\n\\n\\nAdd to an existing collection\\n\\n\\n\\n\\n\\n                Name your collection\\n               *\\n\\n\\n\\n\\n\\n                Choose a collection\\n              \\n\\n\\n\\n                Unable to load your collection due to an error\\nPlease try again\\n\\n\\n\\n\\n\\n          Add\\n        \\n\\n          Cancel\\n        \\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Arxiv Research Paper Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=ArxivLoader(\"2204.13154\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2022-04-27', 'Title': 'Attention Mechanism in Neural Networks: Where it Comes and Where it Goes', 'Authors': 'Derya Soydaner', 'Summary': 'A long time ago in the machine learning literature, the idea of incorporating\\na mechanism inspired by the human visual system into neural networks was\\nintroduced. This idea is named the attention mechanism, and it has gone through\\na long development period. Today, many works have been devoted to this idea in\\na variety of tasks. Remarkable performance has recently been demonstrated. The\\ngoal of this paper is to provide an overview from the early work on searching\\nfor ways to implement attention idea with neural networks until the recent\\ntrends. This review emphasizes the important milestones during this progress\\nregarding different tasks. By this way, this study aims to provide a road map\\nfor researchers to explore the current development and get inspired for novel\\napproaches beyond the attention.'}, page_content='Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepted: 27 April 2022\\nAbstract A long time ago in the machine learning literature, the idea of\\nincorporating a mechanism inspired by the human visual system into neural\\nnetworks was introduced. This idea is named the attention mechanism, and it\\nhas gone through a long development period. Today, many works have been\\ndevoted to this idea in a variety of tasks. Remarkable performance has re-\\ncently been demonstrated. The goal of this paper is to provide an overview\\nfrom the early work on searching for ways to implement attention idea with\\nneural networks until the recent trends. This review emphasizes the impor-\\ntant milestones during this progress regarding diﬀerent tasks. By this way,\\nthis study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.\\nKeywords Attention mechanism · Neural networks · Deep learning · Survey\\n1 Introduction\\nHuman eye sees the world in an interesting way. We suppose as if we see the\\nentire scene at once, but this is an illusion created by the subconscious part\\nof our brain [1]. According to the Scanpath theory [2,3], when the human eye\\nlooks at an image, it can see only a small patch in high resolution. This small\\npatch is called the fovea. It can see the rest of the image in low resolution which\\nis called the periphery. To recognize the entire scene, the eye performs feature\\nextraction based on the fovea. The eye is moved to diﬀerent parts of the image\\nuntil the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner\\nDepartment of Brain and Cognition, University of Leuven (KU Leuven), Leuven, Belgium\\nTel.: +32-16710471\\nE-mail: derya.soydaner@kuleuven.be\\narXiv:2204.13154v1  [cs.LG]  27 Apr 2022\\n2\\nDerya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if it happens all at once.\\nBiologically, this is called visual attention system. Visual attention is de-\\nﬁned as the ability to dynamically restrict processing to a subset of the visual\\nﬁeld [5]. It seeks answers for two main questions: What and where to look?\\nVisual attention has been extensively studied in psychology and neuroscience;\\nfor reviews see [6,7,8,9,10]. Besides, there is a large amount of literature on\\nmodeling eye movements [11,12,13,14]. These studies have been a source of\\ninspiration for many artiﬁcial intelligence tasks. It has been discovered that\\nthe attention idea is useful from image recognition to machine translation.\\nTherefore, diﬀerent types of attention mechanisms inspired from the human\\nvisual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these\\nmechanisms have been integrated into neural networks for a long time.\\nThis survey is about the journey of attention mechanisms used with neu-\\nral networks. Researchers have been investigating ways to strengthen neural\\nnetwork architectures with attention mechanisms for many years. The pri-\\nmary aim of these studies is to reduce computational burden and to improve\\nthe model performance as well. Previous work reviewed the attention mecha-\\nnisms from diﬀerent perspectives [15], or examined them in context of natural\\nlanguage processing (NLP) [16,17]. However, in this study, we examine the\\ndevelopment of attention mechanisms over the years, and recent trends. We\\nbegin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-\\ntention mechanisms. One of them is the Transformer, which is used for many\\nstudies including the GPT-3 language model [18], goes beyond convolutions\\nand recurrence by replacing them with only attention layers [19]. Finally, we\\ndiscuss how much more can we move forward, and what’s next?\\n2 From the Late 1980s to Early 2010s: The Attention Awakens\\nThe ﬁrst attempts at adapting attention mechanisms to neural networks go\\nback to the late 1980s. One of the early studies is the improved version of\\nthe Neocognitron [20] with selective attention [21]. This study is then mod-\\niﬁed to recognize and segment connected characters in cursive handwriting\\n[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-\\ntecture named Signal Channelling Attentional Network (SCAN) is presented\\nfor attentional scanning [23].\\nEarly work on improving the attention idea for neural networks includes\\na variety of tasks such as target detection [24]. In another study, a visual at-\\ntention system extracts regions of interest by combining the bottom-up and\\ntop-down information from the image [25]. A recognition model based on se-\\nlective attention which analyses only a small part of the image at each step,\\nand combines results in time is described [4]. Besides, a model based on the\\nAttention Mechanism in Neural Networks:\\n3\\nconcept of selective tuning is proposed [26]. As the years go by, several studies\\nthat use the attention idea in diﬀerent ways have been presented for visual\\nperception and recognition [27,28,29,30].\\nBy the 2000s, the studies on making attention mechanisms more useful for\\nneural networks continued. In the early years, a model that integrates an at-\\ntentional orienting where pathway and an object recognition what pathway is\\npresented [31]. A computational model of human eye movements is proposed\\nfor an object class detection task [32]. A serial model is presented for visual pat-\\ntern recognition gathering Markov models and neural networks with selective\\nattention on the handwritten digit recognition and face recognition problems\\n[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea\\nis used for object recognition [34], and the analysis of a scene [35]. An inter-\\nesting study proposes to learn sequential attention in real-world visual object\\nrecognition using a Q-learner [36]. Besides, a computational model of visual\\nselective attention is described to automatically detect the most relevant parts\\nof a color picture displayed on a television screen [37]. The attention idea is\\nalso used for identifying and tracking objects in multi-resolution digital video\\nof partially cluttered environments [38].\\nIn 2010, the ﬁrst implemented system inspired by the fovea of human retina\\nwas presented for image classiﬁcation [39]. This system jointly trains a re-\\nstricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-\\nmultaneous object tracking and recognition that is driven by gaze data [40].\\nBy taking advantage of reinforcement learning, a novel recurrent neural net-\\nwork (RNN) is described for image classiﬁcation [41]. Deep Attention Selective\\nNetwork (DasNet), a deep neural network with feedback connections that are\\nlearned through reinforcement learning to direct selective attention to certain\\nfeatures extracted from images, is presented [42]. Additionally, a deep learning\\nbased framework using attention has been proposed for generative modeling\\n[43].\\n3 2015: The Rise of Attention\\nIt can be said that 2015 is the golden year of attention mechanisms. Because\\nthe number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for\\nneural machine translation (NMT) [44]. As it is known, most of the NMT\\nmodels belong to a family of encoder-decoders [45,46], with an encoder and a\\ndecoder for each language. However, compressing all the necessary information\\nof a source sentence into a ﬁxed-length vector is an important disadvantage of\\nthis encoder-decoder approach. This usually makes it diﬃcult for the neural\\nnetwork to capture all the semantic details of a very long sentence [1].\\nThe idea that [44] introduced is an extension to the conventional NMT\\nmodels. This extension is composed of an encoder and decoder as shown in\\n4\\nDerya Soydaner\\nFig. 1 The extension to the conventional NMT models that is proposed by [44]. It generates\\nthe t-th target word yt given a source sentence (x1, x2, ..., xT ).\\nFig 1. The ﬁrst part, encoder, is a bidirectional RNN (BiRNN) [47] that takes\\nword vectors as input. The forward and backward states of BiRNN are com-\\nputed. Then, an annotation aj for each word xj is obtained by concatenating\\nthese forward and backward hidden states. Thus, the encoder maps the input\\nsentence to a sequence of annotations (a1, ..., aTx). By using a BiRNN rather\\nthan conventional RNN, the annotation of each word can summarize both\\nthe preceding words and the following words. Besides, the annotation aj can\\nfocus on the words around xj because of the inherent nature of RNNs that\\nrepresenting recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as\\nin Eq. (1). This neural network f is deﬁned as an alignment model that can\\nbe jointly trained with the proposed architecture. In order to reduce compu-\\ntational burden, a multilayer perceptron (MLP) with a single hidden layer is\\nproposed as f. This alignment model tells us about the relation between the\\ninputs around position j and the output at position i. By this way, the decoder\\napplies an attention mechanism. As it is seen in Eq. (2), the αij is the output\\nof softmax function:\\neij = f(hi−1, aj)\\n(1)\\nαij =\\nexp(eij)\\nPTx\\nk=1 exp(eik)\\n(2)\\nAttention Mechanism in Neural Networks:\\n5\\nHere, the probability αij determines the importance of annotation aj with\\nrespect to the previous hidden state hi−1. Finally, the context vector ci is\\ncomputed as a weighted sum of these annotations as follows [44]:\\nci =\\nTx\\nX\\nj=1\\nαijaj\\n(3)\\nBased on the decoder state, the context and the last generated word, the\\ntarget word yt is predicted. In order to generate a word in a translation, the\\nmodel searches for the most relevant information in the source sentence to\\nconcentrate. When it ﬁnds the appropriate source positions, it makes the pre-\\ndiction. By this way, the input sentence is encoded into a sequence of vectors\\nand a subset of these vectors is selected adaptively by the decoder that is rel-\\nevant to predicting the target [44]. Thus, it is no longer necessary to compress\\nall the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning\\n[48]. Diﬀerent from the previous study [44], it uses a deep convolutional neural\\nnetwork (CNN) as an encoder. This architecture is an extension of the neural\\nnetwork [49] that encodes an image into a compact representation, followed by\\nan RNN that generates a corresponding sentence. Here, the annotation vectors\\nai ∈RD are extracted from a lower convolutional layer, each of which is a D-\\ndimensional representation corresponding to a part of the image. Thus, the\\ndecoder selectively focuses on certain parts of an image by weighting a subset\\nof all the feature vectors [48]. This extended architecture uses attention for\\nsalient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at\\ntime t. The weight αi of each annotation vector is computed similar to Eq. (2),\\nwhereas its associated energy is computed similar to Eq. (1) by using an MLP\\nconditioned on the previous hidden state ht−1. The remarkable point of this\\nstudy is a new mechanism φ that computes ct from the annotation vectors ai\\ncorresponding to the features extracted at diﬀerent image locations:\\nct = φ(\\n\\x08\\nai\\n\\t\\n,\\n\\x08\\nαi\\n\\t\\n)\\n(4)\\nThe deﬁnition of the φ function causes two variants of attention mecha-\\nnisms: The hard (stochastic) attention mechanism is trainable by maximizing\\nan approximate variational lower bound, i.e., by REINFORCE [50]. On the\\nother side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable\\nst, and uses it to decide where to focus attention when generating the t-th\\nword. When the hard attention is applied, the attention locations are con-\\nsidered as intermediate latent variables. It assigns a multinoulli distribution\\nparametrized by αi, and ct becomes a random variable. Here, st,i is deﬁned\\nas a one-hot variable which is set to 1 if the i-th location is used to extract\\nvisual features [48]:\\n6\\nDerya Soydaner\\np(st,i = 1|sj<t, a) = αt,i\\n(5)\\nct =\\nX\\ni\\nst,iai\\n(6)\\nWhereas learning hard attention requires sampling the attention location\\nst each time, the soft attention mechanism computes a weighted annotation\\nvector similar to [44] and takes the expectation of the context vector ct directly:\\nEp(st|α)[ct] =\\nL\\nX\\ni=1\\nαt,iai\\n(7)\\nFurthermore, in training the deterministic version of the model, an alterna-\\ntive method namely doubly stochastic attention, is proposed with an additional\\nconstraint added to the training objective to encourage the model to pay equal\\nattention to all parts of the image.\\nThe third study should be emphasized presents two classes of attention\\nmechanisms for NMT: the global attention that always attends to all source\\nwords, and the local attention that only looks at a subset of source words at\\na time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the\\nlocal one selectively focuses on a small window of context. In global attention, a\\nvariable-length alignment vector is derived similar to Eq. (2). Here, the current\\ntarget hidden state ht is compared with each source hidden state ¯hs by using a\\nscore function instead of the associated energy eij. Thus, the alignment vector\\nwhose size equals the number of time steps on the source side is derived.\\nGiven the alignment vector as weights, the context vector ct is computed as\\nthe weighted average over all the source hidden states. Here, score is referred\\nas a content-based function, and three diﬀerent alternatives are considered [51].\\nOn the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window\\ncentered around the source position pt is used to compute the context vector\\nas a weighted average of the source hidden states within the window. The\\nlocal attention selectively focuses on a small window of context, and obtains\\nthe alignment vector from the current target state ht and the source states ¯hs\\nin the window [51].\\nThe introduction of these novel mechanisms in 2015 triggered the rise of\\nattention for neural networks. Based on the proposed attention mechanisms,\\nsigniﬁcant research has been conducted in a variety of tasks. In order to imag-\\nine the attention idea in neural networks better, two visual examples are shown\\nin Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows\\nhow the attention mechanisms can be used for visual question answering [52].\\nBoth examples demonstrate how attention mechanisms focus on parts of input\\nimages.\\nAttention Mechanism in Neural Networks:\\n7\\nFig. 2 Examples of the attention mechanism in visual. (Top) Attending to the correct\\nobject in neural image caption generation [48]. (Bottom) Visualization of original image\\nand question pairs, and co-attention maps namely word-level, phrase-level and question-\\nlevel, respectively [52].\\n4 2015-2016: Attack of the Attention\\nDuring two years from 2015, the attention mechanisms were used for diﬀerent\\ntasks, and novel neural network architectures were presented applying these\\nmechanisms. After the memory networks [53] that require a supervision signal\\ninstructing them how to use their memory cells, the introduction of the neural\\nTuring machine [54] allows end-to-end training without this supervision signal,\\nvia the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.\\nIn these years, an attention mechanism called self-attention, sometimes\\ncalled intra-attention, was successfully implemented within a neural network\\narchitecture namely Long Short-Term Memory-Networks (LSTMN) [56]. It\\nmodiﬁes the standard LSTM structure by replacing the memory cell with a\\nmemory network [53]. This is because memory networks have a set of key\\nvectors and a set of value vectors, whereas LSTMs maintain a hidden vector\\nand a memory vector [56]. In contrast to attention idea in [44], memory and\\nattention are added within a sequence encoder in LSTMN. In order to compute\\na representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].\\nMany attention-based models have been proposed for neural image cap-\\ntioning [58], abstractive sentence summarization [59], speech recognition [60,\\n61], automatic video captioning [62], neural machine translation [63], and rec-\\nognizing textual entailment [64]. Diﬀerent attention-based models perform vi-\\nsual question answering [65,66,67]. An attention-based CNN is presented for\\nmodeling sentence pairs [68]. A recurrent soft attention based model learns to\\nfocus selectively on parts of the video frames and classiﬁes videos [69].\\nOn the other side, several neural network architectures have been pre-\\nsented in a variety of tasks. For instance, Stacked Attention Network (SAN)\\n8\\nDerya Soydaner\\nis described for image question answering [70]. Deep Attention Recurrent Q-\\nNetwork (DARQN) integrates soft and hard attention mechanisms into the\\nstructure of Deep Q-Network (DQN) [71]. Wake-Sleep Recurrent Attention\\nModel (WS-RAM) speeds up the training time for image classiﬁcation and\\ncaption generation tasks [72]. alignDRAW model, an extension of the Deep\\nRecurrent Attention Writer (DRAW) [73], is a generative model of images\\nfrom captions using a soft attention mechanism [74]. Generative Adversarial\\nWhat-Where Network (GAWWN) synthesizes images given instructions de-\\nscribing what content to draw in which location [75].\\n5 The Transformer: Return of the Attention\\nAfter the proposed attention mechanisms in 2015, researchers published stud-\\nies that mostly modifying or implementing them to diﬀerent tasks. However,\\nin 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great\\nresults on two machine translation tasks in addition to English constituency\\nparsing. The most impressive point about this architecture is that it contains\\nneither recurrence nor convolution. The Transformer performs well by replac-\\ning the conventional recurrent layers in encoder-decoder architecture used for\\nNMT with self-attention.\\nThe Transformer is composed of encoder-decoder stacks each of which has\\nsix identical layers within itself. In Fig. 3, one encoder-decoder stack is shown\\nto illustrate the model [19]. Each stack includes only attention mechanisms\\nand feedforward neural networks. As this architecture does not include any\\nrecurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.\\nThe calculations of self-attention are slightly diﬀerent from the mechanisms\\ndescribed so far in this paper. It uses three vectors namely query, key and\\nvalue for each word. These vectors are computed by multiplying the input with\\nweight matrices Wq, Wk and Wv which are learned during training. In general,\\neach value is weighted by a function of the query with the corresponding key.\\nThe output is computed as a weighted sum of the values. Based on this idea,\\ntwo attention mechanisms are proposed: In the ﬁrst one, called scaled dot-\\nproduct attention, the dot products of the query with all keys are computed\\nas given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the\\nsoftmax function, thus the weights for the values are obtained. Finally each\\nsoftmax score is multiplied with the value as given in Eq. (8). The authors\\npropose computing the attention on a set of queries simultaneously by taking\\nqueries and keys of dimension dk, and values of dimension dv as inputs. The\\nkeys, queries and values are packed together into matrices K, Q and V. Finally,\\nthe output matrix is obtained as follows [19]:\\nAttention Mechanism in Neural Networks:\\n9\\nFig. 3 The Transformer architecture and the attention mechanisms it uses in detail [19].\\n(Left) The Transformer with one encoder-decoder stack. (Center) Multi-head attention.\\n(Right) Scaled dot-product attention.\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(8)\\nThis calculation is performed by every word against the other words. This\\nleads to having values of each word relative to each other. For instance, if\\nthe word x2 is not relevant for the word x1, then the softmax score gives low\\nprobability scores. As a result, the corresponding value is decreased. This leads\\nto an increase in the value of relevant words, and those of others decrease. In\\nthe end, every word obtains a new value for itself.\\nAs seen from Fig. 3, the Transformer model does not directly use scaled\\ndot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,\\nlinearly projects the queries, keys and values h times with diﬀerent, learned\\nlinear projections to dq, dk and dv dimensions, respectively [19]. The attention\\nfunction is performed in parallel on each of these projected versions of queries,\\nkeys and values, i.e., heads. By this way, dv-dimensional output values are\\nobtained. In order to get the ﬁnal values, they are concatenated and projected\\none last time as shown in the center of Fig. 3. By this way, the self-attention is\\ncalculated multiple times using diﬀerent sets of query, key and value vectors.\\nThus, the model can jointly attend to information at diﬀerent positions [19]:\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\n(9)\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is\\napplied ﬁrst to ensure that only previous word embeddings are used when\\ntrying to predict the next word in the sentence. Therefore, the embeddings\\nthat shouldn’t be seen by the decoder are masked by multiplying with zero.\\n10\\nDerya Soydaner\\nAn interesting study examines the contribution made by individual atten-\\ntion heads in the encoder [76]. Also, there is an evaluation of the eﬀects of\\nself-attention on gradient propagation in recurrent networks [77]. For a deeper\\nanalysis of multi-head self-attention mechanism from a theoretical perspective\\nsee [78].\\nSelf-attention has been used successfully in a variety of tasks including\\nsentence embedding [79] and abstractive summarization [80]. It is shown that\\nself-attention can lead to improvements to discriminative constituency parser\\n[81], and speech recognition as well [82,83]. Also, the listen-attend-spell model\\n[84] has been improved with the self-attention for acoustic modeling [85].\\nAs soon as these self-attention mechanisms were proposed, they have been\\nincorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-\\nmains with the aid of self-attention mechanism [86]. Novel self-attention neural\\nmodels are proposed for cross-target stance classiﬁcation [87] and NMT [88].\\nAnother study points out that a fully self-attentional model can reach com-\\npetitive predictive performance on ImageNet classiﬁcation and COCO object\\ndetection tasks [89]. Besides, developing novel attention mechanisms has been\\ncarried out such as area attention, a novel mechanism that can be used along\\nmulti-head attention [90]. It attends to areas in the memory by deﬁning the\\nkey of an area as the mean vector of the key of each item, and deﬁning the\\nvalue as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks\\n(SAGANs) [92] introduce a self-attention mechanism into convolutional GANs.\\nDiﬀerent from the traditional convolutional GANs, SAGAN generates high-\\nresolution details using cues from all feature locations. Similarly, Attentional\\nGenerative Adversarial Network (AttnGAN) is presented for text to image\\ngeneration [93]. On the other side, a machine reading and question answering\\narchitecture called QANet [94] is proposed without any recurrent networks. It\\nuses self-attention to learn the global interaction between each pair of words\\nwhereas convolution captures the local structure of the text. In another study,\\nGated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces\\nattentive group convolutions with a generalization of visual self-attention [96].\\nA deep transformer model is implemented for language modeling over long\\nsequences [97].\\n5.1 Self-attention variants\\nIn recent years, self-attention has become an important research direction\\nwithin the deep learning community. Self-attention idea has been examined\\nin diﬀerent aspects. For example, self-attention is handled in a multi-instance\\nlearning framework [98]. The idea of Sparse Adaptive Connection (SAC) is\\npresented for accelerating and structuring self-attention [99]. The research on\\nAttention Mechanism in Neural Networks:\\n11\\nTable 1 Summary of Notation\\nSymbol\\nDeﬁnition\\na\\nannotation\\nc\\ncontext vector\\nα\\nweight\\ne\\nenergy\\nf\\nfeedforward neural network\\nh\\nhidden state\\nφ\\nhard (stochastic) / soft (deterministic) attention\\ns\\nlocation variable\\np\\nsource position\\nK, Q, V\\nkeys, queries and values matrices, respectively\\nWq, Wk, Wv\\nweight matrices for queries, keys and values, respectively\\nimproving self-attention continues as well [100,101,102]. Besides, based on the\\nself-attention mechanisms proposed in the Transformer, important studies that\\nmodify the self-attention have been presented. Some of the most recent and\\nprominent studies are summarized below.\\nRelation-aware self-attention It extends the self-attention mechanism by\\nregarding representations of the relative positions, or distances between se-\\nquence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent\\nthe edge between two inputs. It provides learning two distinct edge represen-\\ntations that can be shared across attention heads without requiring additional\\nlinear transformations.\\nDirectional self-attention (DiSA) A novel neural network architecture\\nfor learning sentence embedding named Directional Self-Attention Network\\n(DiSAN) [104] uses directional self-attention followed by a multi-dimensional\\nattention mechanism. Instead of computing a single importance score for each\\nword based on the word embedding, multi-dimensional attention computes a\\nfeature-wise score vector for each token. To extend this mechanism to the self-\\nattention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.\\nThe second one, called multi-dimensional ‘source2token’ self-attention com-\\npresses the sequence into a vector [104]. On the other side, directional self-\\nattention produces context-aware representations with temporal information\\nencoded by using positional masks. By this way, directional information is en-\\ncoded. First, the input sequence is transformed to a sequence of hidden states\\nby a fully connected layer. Then, multi-dimensional token2token self-attention\\nis applied to these hidden states. Hence, context-aware vector representations\\nare generated for all elements from the input sequence.\\nReinforced self-attention (ReSA) A sentence-encoding model named Re-\\ninforced Self-Attention Network (ReSAN) uses reinforced self-attention (ReSA)\\nthat integrates soft and hard attention mechanisms into a single model. ReSA\\n12\\nDerya Soydaner\\nselects a subset of head tokens, and relates each head token to a small sub-\\nset of dependent tokens to generate their context-aware representations [105].\\nFor this purpose, a novel hard attention mechanism called reinforced sequence\\nsampling (RSS), which selects tokens from an input sequence in parallel and\\ntrained via policy gradient, is proposed. Given an input sequence, RSS gener-\\nates an equal-length sequence of binary random variables that indicates both\\nthe selected and discarded ones. On the other side, the soft attention provides\\nreward signals back for training the hard attention. The proposed RSS pro-\\nvides a sparse mask to self-attention. ReSA uses two RSS modules to extract\\nthe sparse dependencies between each pair of selected tokens.\\nOuter product attention (OPA) Self-Attentive Associative Memory (SAM)\\nis a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers\\nusing element-wise multiplication, outer product, and tanh function instead of\\nsoftmax.\\nBidirectional block self-attention (Bi-BloSA) Another mechanism, bidi-\\nrectional block self-attention (Bi-BloSA) which is simply a masked block self-\\nattention (mBloSA) with forward and backward masks to encode the tempo-\\nral order information is presented [107]. Here, mBloSA is composed of three\\nparts from its bottom to top namely intra-block self-attention, inter-block\\nself-attention and the context fusion. It splits a sequence into several length-\\nequal blocks, and applies an intra-block self-attention to each block indepen-\\ndently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-\\nory compared to a single one applied to the whole sequence. Finally, a feature\\nfusion gate combines the outputs of intra-block and inter-block self-attention\\nwith the original input, to produce the ﬁnal context-aware representations of\\nall tokens.\\nFixed multi-head attention The ﬁxed multi-head attention proposes ﬁxing\\nthe head size of the Transformer in the aim of improving the representation\\npower [108]. This study emphasizes its importance by setting the head size of\\nattention units to input sequence length.\\nSparse sinkhorn attention It is based on the idea of diﬀerentiable sorting\\nof internal representations within the self-attention module [109]. Instead of\\nallowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.\\nThus, tokens that may be far apart in the unsorted sequence can be considered.\\nAdditionally, a variant of this mechanism named SortCut sinkhorn attention\\napplies a post-sorting truncation of the input sequence.\\nAttention Mechanism in Neural Networks:\\n13\\nAdaptive attention span Adaptive attention span is proposed as an alter-\\nnative to self-attention [110]. It learns the attention span of each head inde-\\npendently. To this end, a masking function inspired by [111] is used to control\\nthe attention span for each head. The purpose of this novel mechanism is to\\nreduce the computational burden of the Transformer. Additionally, dynamic\\nattention span approach is presented to dynamically change the attention span\\nbased on the current input as an extension [51,112].\\n5.2 Transformer variants\\nDiﬀerent from developing novel self-attention mechanisms, several studies have\\nbeen published in the aim of improving the performance of the Transformer.\\nThese studies mostly modify the model architecture. For instance, an addi-\\ntional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to\\nimprove Transformer optimization [114]. A novel positional encoding scheme\\nis used to extend the Transformer to tree-structured data [115]. Investigating\\nmodel size by handling Transformer width and depth for eﬃcient training is\\nalso an active research area [116]. Transformer is used in reinforcement learn-\\ning settings [117,118,119] and for time series forecasting in adversarial training\\nsetting [120].\\nBesides, many Transformer variants have been presented in the recent past.\\nCOMmonsEnse Transformer (COMET) is introduced for automatic construc-\\ntion of commonsense knowledge bases [121]. Evolved Transformer applies neu-\\nral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].\\nCrossTransformer takes a small number of labeled images and an unlabeled\\nquery, and computes distances between spatially-corresponding features to in-\\nfer class membership [124]. DEtection TRansformer (DETR) is a new design\\nfor object detection systems [125], and Deformable DETR is an improved ver-\\nsion that achieves better performance in less time [126]. FLOw-bAsed Trans-\\nformER (FLOATER) emphasizes the importance of position encoding in the\\nTransformer, and models the position information via a continuous dynamical\\nmodel [127]. Disentangled Context (DisCo) Transformer simultaneously gener-\\nates all tokens given diﬀerent contexts by predicting every word in a sentence\\nconditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative\\nmodeling [129].\\nRecent work has demonstrated signiﬁcant performance on NLP tasks. In\\nOpenAI GPT, there is a left-to-right architecture, where every token can only\\nattend to previous tokens in the self-attention layers of the Transformer [130].\\nGPT-2 [131] and GPT-3 [18] models have improved the progress. In addition\\nto these variants, some prominent Transformer-based models are summarized\\nbelow.\\n14\\nDerya Soydaner\\nUniversal Transformer A generalization of the Transformer model named\\nthe Universal Transformer [132] iteratively computes representations Ht at\\nstep t for all positions in the sequence in parallel. To this end, it uses the\\nscaled dot-product attention in Eq. (8) where d is the number of columns\\nof Q, K and V. In the Universal Transformer, the multi-head self-attention\\nwith k heads is used. The representations Ht is mapped to queries, keys and\\nvalues with aﬃne projections using learned parameter matrices W Q ∈ℜd×d/k,\\nW K ∈ℜd×d/k, W V ∈ℜd×d/k and W O ∈ℜd×d [132]:\\nMultiHead(Ht) = Concat(head1, ..., headk)W O\\n(10)\\nwhere headi = Attention(HtW Q\\ni , HtW K\\ni , HtW V\\ni )\\nImage Transformer Image Transformer [133] demonstrates that self-attention\\nbased models can also be well-suited for images instead of text. This Trans-\\nformer type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.\\nIts larger receptive ﬁelds allow the Image Transformer to signiﬁcantly improve\\nthe model performance on image generation as well as image super-resolution.\\nTransformer-XL This study aims to improve the ﬁxed-length context of the\\nTransformer [19] for language modeling. Transformer-XL [134] makes model-\\ning very long-term dependency possible by reusing the hidden states obtained\\nin previous segments. Hence, information can be propagated through the recur-\\nrent connections. In order to reuse the hidden states without causing temporal\\nconfusion, Transformer-XL uses relative positional encodings. Based on this\\narchitecture, a modiﬁed version named the Gated Transformer-XL (GTrXL)\\nis presented in the reinforcement learning setting [135].\\nTensorized Transformer Tensorized Transformer [136] compresses the multi-\\nhead attention in Transformer. To this end, it uses a novel self-attention model\\nmulti-linear attention with Block-Term Tensor Decomposition (BTD) [137]. It\\nbuilds a single-block attention based on the Tucker decomposition [138]. Then,\\nit uses a multi-linear attention constructed by a BTD to compress the multi-\\nhead attention mechanism. In Tensorized Transformer, the factor matrices are\\nshared across multiple blocks.\\nBERT The Bidirectional Encoder Representations from Transformers (BERT)\\naims to pre-train deep bidirectional representations from unlabeled text [139].\\nBERT uses a multilayer bidirectional Transformer as the encoder. Besides,\\ninspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-\\ndicts the original vocabulary id of the masked word based only on its context.\\nThis model can pre-train a deep bidirectional Transformer. In all layers, the\\nAttention Mechanism in Neural Networks:\\n15\\npre-training is carried out by jointly conditioning on both left and right con-\\ntext. BERT diﬀers from the left-to-right language model pre-training from this\\naspect.\\nRecently, BERT model has been examined in detail. For instance, the be-\\nhaviour of attention heads are analysed [141]. Various methods have been\\ninvestigated for compressing [142,143], pruning [144], and quantization [145].\\nAlso, BERT model has been considered for diﬀerent tasks such as coreference\\nresolution [146]. A novel method is proposed in order to accelerate BERT\\ntraining [147].\\nFurthermore, various BERT variants have been presented. ALBERT aims\\nto increase the training speed of BERT, and presents two parameter reduction\\ntechniques [148]. Similarly, PoWER-BERT [149] is developed to improve the\\ninference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while\\nmaintaining accuracy [150]. In order to obtain better representations, Span-\\nBERT is proposed as a pre-training method [151]. As a robustly optimized\\nBERT approach, RoBERTa shows that BERT was signiﬁcantly undertrained\\n[152]. Also, DeBERTa improves RoBERTa using the disentangled attention\\nmechanism [153]. On the other side, DistilBERT shows that it is possible to\\nreach similar performances using much smaller language models pre-trained\\nwith knowledge distillation [154]. StructBERT proposes two novel lineariza-\\ntion strategies [155]. Q-BERT is introduced for quantizing BERT models [156],\\nBioBERT is for biomedical text mining [157], and RareBERT is for rare dis-\\nease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-\\nformer for various tasks, or making them more understandable. In one of the\\nmost recent studies, NLP becomes possible in the mobile setting with Lite\\nTransformer. It applies long-short range attention where some heads specialize\\nin the local context modeling while the others specialize in the long-distance\\nrelationship modeling [159]. A deep and light-weight Transformer DeLighT\\n[160] and a hypernetwork-based model namely HyperGrid Transformers [161]\\nperform with fewer parameters. Graph Transformer Network is introduced\\nfor learning node representations on heterogeneous graphs [162] and diﬀerent\\napplications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured\\ntext data [165]. AttentionXML is a tree-based model for extreme multi-label\\ntext classiﬁcation [166]. Besides, attention mechanism is handled in a Bayesian\\nframework [167]. For a better understanding of Transformers, an identiﬁabil-\\nity analysis of self-attention weights is conducted in addition to presenting\\neﬀective attention to improve explanatory interpretations [168]. Lastly, Vision\\nTransformer (ViT) processes an image using a standard Transformer encoder\\nas used in NLP by interpreting it as a sequence of patches, and performs well\\non image classiﬁcation tasks [169].\\n16\\nDerya Soydaner\\n5.3 What about complexity?\\nAll these aforementioned studies undoubtedly demonstrate signiﬁcant success.\\nBut success not make one great. The Transformer also brings a very high\\ncomputational complexity and memory cost. The necessity of storing atten-\\ntion matrix to compute the gradients with respect to queries, keys and val-\\nues causes a non-negligible quadratic computation and memory requirements.\\nTraining the Transformer is a slow process for very long sequences because\\nof its quadratic complexity. There is also time complexity which is quadratic\\nwith respect to the sequence length. In order to improve the Transformer in\\nthis respect, recent studies have been conducted to improve this issue. One\\nof them is Linear Transformer which expresses the self-attention as a linear\\ndot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax\\nfunction in Eq. (8) to a feature map based dot-product attention. Its per-\\nformance is competitive with the vanilla Transformer architecture on image\\ngeneration and automatic speech recognition tasks while being faster during\\ninference. On the other side, FMMformers which use the idea of the fast multi-\\npole method (FMM) [171] outperform the linear Transformer by decomposing\\nthe attention matrix into near-ﬁeld and far-ﬁeld attention with linear time and\\nmemory complexity [172].\\nAnother suggestion made in response to the Transformer’s quadratic na-\\nture is The Reformer that replaces dot-product attention by one that uses\\nlocality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set\\nTransformer aims to reduce computation time of self-attention from quadratic\\nto linear by using an attention mechanism based on sparse Gaussian process\\nliterature [174]. Routing Transformer aims to reduce the overall complexity\\nof attention by learning dynamic sparse attention patterns by using routing\\nattention with clustering [175]. It applies k-means clustering to model sparse\\nattention matrices. At ﬁrst, queries and keys are assigned to clusters. The at-\\ntention scheme is determined by considering only queries and keys from the\\nsame cluster. Thus, queries are routed to keys belonging to the same cluster\\n[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-\\nputational burden [176]. It also shows the possibility of modeling sequences\\nof length one million or more by using self-attention in theory. In the Trans-\\nformer, all the attention heads with the softmax attention assign a non-zero\\nweight to all context words. Adaptively Sparse Transformer replaces softmax\\nwith α-entmax which is a diﬀerentiable generalization of softmax allowing\\nlow-scoring words to receive precisely zero weight [177]. By means of context-\\ndependent sparsity patterns, the attention heads become ﬂexible in the Adap-\\ntively Sparse Transformer. Random feature attention approximates softmax\\nattention with random feature methods [178]. Skyformer replaces softmax\\nwith a Gaussian kernel and adapts Nystr¨om method [179]. A sparse atten-\\nAttention Mechanism in Neural Networks:\\n17\\ntion mechanism named BIGBIRD aims to reduce the quadratic dependency\\nof Transformer-based models to linear [180]. Diﬀerent from the similar stud-\\nies, BIGBIRD performs well for genomics data alongside NLP tasks such as\\nquestion answering.\\nMusic Transformer [181] shows that self-attention can also be useful for\\nmodeling music. This study emphasizes the infeasibility of the relative po-\\nsition representations introduced by [103] for long sequences because of the\\nquadratic intermediate relative information in the sequence length. Therefore,\\nthis study presents an extended version of relative attention named relative\\nlocal attention that improves the relative attention for longer musical com-\\npositions by reducing its intermediate memory requirement to linear in the\\nsequence length. A softmax-free Transformer (SOFT) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead\\nof the dot-product similarity [182].\\nAdditionally, various approaches have been presented in Hierarchical Vi-\\nsual Transformer [183], Long-Short Transformer (Transformer-LS) [184], Per-\\nceiver [185], and Performer [186]. Image Transformer based on the cross-\\ncovariance matrix between keys and queries is applied [187], and a new vi-\\nsion Transformer is proposed [188]. Furthermore, a Bernoulli sampling atten-\\ntion mechanism decreases the quadratic complexity to linear [189]. A novel\\nlinearized attention mechanism performs well on object detection, instance\\nsegmentation, and stereo depth estimation [190]. A study shows that kernel-\\nized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long\\nsequences [191]. A linear uniﬁed nested attention mechanism namely Luna\\nuses two nested attention functions to approximate the softmax attention in\\nTransformer to achieve linear time and space complexity [192].\\n6 Concluding Remarks: A New Hope\\nInspired by the human visual system, the attention mechanisms in neural net-\\nworks have been developing for a long time. In this study, we examine this\\nduration beginning with its roots up to the present time. Some mechanisms\\nhave been modiﬁed, or novel mechanisms have emerged in this period. Today,\\nthis journey has reached a very important stage. The idea of incorporating\\nattention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family\\nmodels have become a new hope for more advanced models. These promising\\nprogress bring the questions whether the attention could help further devel-\\nopment, replace the popular neural network layers, or could be a better idea\\nthan the existing attention mechanisms? It is still an active research area and\\nmuch to learn we still have, but it is obvious that more powerful systems are\\nawaiting when neural networks and attention mechanisms join forces.\\n18\\nDerya Soydaner\\nConﬂict of interest\\nThe author declares that she has no conﬂict of interest.\\nReferences\\n1. I. Goodfellow, Y. Bengio, A. Courville, The MIT Press (2016)\\n2. D. Noton, L. Stark, Scientiﬁc American 224(6), 34 (1971)\\n3. D. Noton, L. Stark, Vision Research 11, 929 (1971)\\n4. E. Alpaydın, Advances in Neural Information Processing Systems 8 pp. 771–777 (1995)\\n5. S. Ahmad, Advances in Neural Information Processing Systems 4 pp. 420–427 (1991)\\n6. M. Posner, S. Petersen, Annual Review of Neuroscience 13(1), 25 (1990)\\n7. C. Bundesen, Psychological Review 97(4), 523 (1990)\\n8. R. Desimone, J. Duncan, Annual Review of Neuroscience 18(1), 193 (1995)\\n9. M. Corbetta, G. Shulman, Nature Reviews Neuroscience 3(3), 201 (2002)\\n10. S. Petersen, M. Posner, Annual Review of Neuroscience 35, 73 (2012)\\n11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)\\n13. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 105(2), 261 (1995)\\n14. J. Hoﬀman, B. Subramaniam, Perception and Psychophysics 57(6), 787 (1995)\\n15. S. Chaudhari, et al., ACM Transactions on Intelligent Systems and Technology (TIST)\\npp. 1–32 (2021)\\n16. A. Galassi, et al., IEEE Transactions on Neural Networks and Learning Systems (2020)\\n17. J. Lee, et al., ACM Transactions on Knowledge Discovery from Data (TKDD) 13(6),\\n1 (2019)\\n18. T. Brown, et al., Advances in Neural Information Processing Systems 33 pp. 1877–1901\\n(2020)\\n19. A. Vaswani, et al., Advances in Neural Information Processing Systems 30 pp. 5998–\\n6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)\\n23. E. Postma, H.V. den Herik, P. Hudson, Neural Networks 10(6), 993 (1997)\\n24. J. Schmidhuber, R. Huber, International Journal of Neural Systems pp. 125–134 (1991)\\n25. R. Milanese, et al., IEEE Computer Society Conference on Computer Vision and Pat-\\ntern Recoginition, Seattle, WA, USA pp. 781–785 (1994)\\n26. J. Tsotsos, et al., Artiﬁcial Intelligence 78(1-2), 507 (1995)\\n27. S. Culhane, J. Tsotsos, Proceedings of the 11th IAPR International Conference on\\nPattern Recognition, The Hague, Netherlands pp. 36–40 (1992)\\n28. D. Reisfeld, H. Wolfson, Y. Yeshurun, International Journal of Computer Vision 14(2),\\n119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)\\n31. F. Miau, L. Itti, Proceedings of the 23rd Annual International Conference of the IEEE\\nEngineering in Medicine and Biology Society, Istanbul, Turkey pp. 789–792 (2001)\\n32. W. Zhang, et al., Advances in Neural Information Processing Systems 19 pp. 1609–1616\\n(2006)\\n33. A. Salah, E. Alpaydın, L. Akarun, IEEE Transactions on Pattern Analysis and Machine\\nIntelligence 24(3), 420 (2002)\\n34. D. Walther, et al., International Workshop on Biologically Motivated Computer Vision,\\nSpringer, Berlin, Heidelberg pp. 472–479 (2002)\\n35. K. Schill, et al., Journal of Electronic Imaging 10(1), 152 (2001)\\n36. L. Paletta, G. Fritz, C. Seifert, International Conference on Machine Learning (2005)\\n37. O.L. Meur, et al., IEEE Transactions on Pattern Analysis and Machine Intelligence\\n28(5), 802– (2006)\\nAttention Mechanism in Neural Networks:\\n19\\n38. S. Gould, et al., International Joint Conference on Artiﬁcial Intelligence (IJCAI) pp.\\n2115–2121 (2007)\\n39. H. Larochelle, G. Hinton, Advances in Neural Information Processing Systems 23 pp.\\n1243–1251 (2010)\\n40. L. Bazzani, et al., International Conference on Machine Learning (2011)\\n41. V. Mnih, et al., Advances in Neural Information Processing Systems 27 pp. 2204–2212\\n(2014)\\n42. M. Stollenga, et al., Advances in Neural Information Processing Systems 27 pp. 3545–\\n3553 (2014)\\n43. Y. Tang, N. Srivastava, R. Salakhutdinov, Advances in Neural Information Processing\\nSystems 27 (2014)\\n44. D. Bahdanau, K. Cho, Y. Bengio, International Conference on Learning Representa-\\ntions (2015)\\n45. I. Sutskever, O. Vinyals, Q. Le, Advances in Neural Information Processing Systems\\n27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)\\n47. M. Schuster, K. Paliwal, IEEE Transactions on Signal Processing 45(11), 2673 (1997)\\n48. K. Xu, et al., International Conference on Machine Learning pp. 2048–2057 (2015)\\n49. O. Vinyals, et al., In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition pp. 3156–3164 (2015)\\n50. R. Williams, Machine Learning 8(3-4), 229 (1992)\\n51. M.T. Luong, H.P..C. Manning, Proceedings of the 2015 Conference on Empirical Meth-\\nods in Natural Language Processing, Lisbon, Portugal pp. 1412–1421 (2015)\\n52. J. Lu, et al., Advances in Neural Information Processing Systems 29 (2016)\\n53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)\\n55. S. Sukhbaatar, et al., Advances in Neural Information Processing Systems 28 pp. 2440–\\n2448 (2015)\\n56. J. Cheng, L. Dong, M. Lapata, Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing pp. 551–561 (2016)\\n57. A. Parikh, et al., Proceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing, Austin, Texas pp. 2249–2255 (2016)\\n58. Q. You, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV pp. 4651–4659 (2016)\\n59. A. Rush, S. Chopra, J. Weston, Proceedings of the 2015 Conference on Empirical\\nMethods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)\\n61. J. Chorowski, et al., Advances in Neural Information Processing Systems 28 pp. 577–\\n585 (2015)\\n62. M. Zanﬁr, E. Marinoiu, C. Sminchisescu, In Asian Conference on Computer Vision,\\nSpringer, Cham pp. 104—-119 (2016)\\n63. Y. Cheng, et al., Proceedings of the 25th International Joint Conference on Artiﬁcial\\nIntelligence (2016)\\n64. T. Rockt International Conference on Learning Representations (2016)\\n65. Y. Zhu, et al., Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 4995–5004 (2016)\\n66. K. Chen, et al., arXiv preprint arXiv:1511.05960 (2015)\\n67. H. Xu, K. Saenko, In European Conference on Computer Vision pp. 451–466 (2016)\\n68. W. Yin, et al., Transactions of the Association for Computational Linguistics 4, 259\\n(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)\\n70. Z. Yang, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 21–29 (2016)\\n71. I. Sorokin, et al., arXiv preprint arXiv:1512.01693 (2015)\\n72. J. Ba, et al., Advances in Neural Information Processing Systems 28 pp. 2593–2601\\n(2015)\\n20\\nDerya Soydaner\\n73. K. Gregor, et al., International Conference on Machine Learning pp. 1462–1471 (2015)\\n74. E. Mansimov, et al., International Conference on Learning Representations (2016)\\n75. S. Reed, et al., Advances in Neural Information Processing Systems 29 pp. 217–225\\n(2016)\\n76. E. Voita, et al., In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 5797–5808 (2019)\\n77. G. Kerg, et al., Advances in Neural Information Processing Systems 33 (2020)\\n78. J.B. Cordonnier, A. Loukas, M. Jaggi, International Conference on Learning Repre-\\nsentations (2020)\\n79. Z. Lin, et al., International Conference on Learning Representations (2017)\\n80. R. Paulus, C. Xiong, R. Socher, International Conference on Learning Representations\\n(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)\\n82. D. Povey, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), IEEE pp. 5874–5878 (2018)\\n83. A. Vyas, et al., Advances in Neural Information Processing Systems 33 (2020)\\n84. W. Chan, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), Shanghai pp. 4960—-4964 (2016)\\n85. M. Sperber, et al., In proceedings of Annual Conference of the International Speech\\nCommunication Association (InterSpeech) pp. 3723–3727 (2018)\\n86. L. Kaiser, et al., arXiv preprint arXiv:1706.05137 (2017)\\n87. C. Xu, et al., Proceedings of the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)\\n89. P. Ramachandran, et al., Advances in Neural Information Processing Systems 32 pp.\\n68–80 (2019)\\n90. Y. Li, et al., International Conference on Machine Learning (2019)\\n91. I. Goodfellow, et al., Advances in Neural Information Processing Systems 27 pp. 2672–\\n2680 (2014)\\n92. H. Zhang, et al., International Conference on Machine Learning pp. 7354–7363 (2019)\\n93. T. Xu, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) pp. 1316–1324 (2018)\\n94. A. Yu, et al., International Conference on Learning Representations (2018)\\n95. J. Zhang, et al., Conference on Uncertainty in Artiﬁcial Intelligence (2018)\\n96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)\\n98. J. Du, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing pp. 2216–2225 (2018)\\n99. X. Li, et al., Advances in Neural Information Processing Systems 33 (2020)\\n100. B. Yang, et al., AAAI Conference on Artiﬁcial Intelligence 33, 387 (2019)\\n101. B. Yang, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, Brussels, Belgium pp. 4449–4458 (2018)\\n102. Proceedings of the IEEE International Conference on Computer Vision pp. 3286–3295\\n103. P. Shaw, J. Uszkoreit, A. Vaswani, Proceedings of NAACL-HLT, New Orleans,\\nLouisiana pp. 464–468 (2018)\\n104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial\\nIntelligence, (IJCAI-18) pp. 4345–4352 (2018)\\n106. H. Le, T. Tran, S. Venkatesh, International Conference on Machine Learning (2020)\\n107. T. Shen, et al., International Conference on Learning Representations (2018)\\n108. S. Bhojanapalli, et al., International Conference on Machine Learning (2020)\\n109. Y. Tay, et al., International Conference on Machine Learning (2020)\\n110. S. Sukhbaatar, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 331–335 (2019)\\n111. Y. Jernite, et al., International Conference on Learning Representations (2017)\\n112. R. Shu, H. Nakayama, In Proceedings of the First Workshop on Neural Machine Trans-\\nlation, Vancouver, Canada pp. 1–10 (2017)\\nAttention Mechanism in Neural Networks:\\n21\\n113. J. Hao, et al., Proceedings of NAACL-HLT, Minneapolis, Minnesota pp. 1198–1207\\n(2019)\\n114. X. Huang, et al., International Conference on Machine Learning (2020)\\n115. V. Shiv, C. Quirk, Advances in Neural Information Processing Systems 32 pp. 12,081–\\n12,091 (2019)\\n116. Z. Li, et al., International Conference on Machine Learning (2020)\\n117. Y. Hoshen, Advances in Neural Information Processing Systems 30, Long Beach, CA,\\nUSA (2017)\\n118. S. Hu, et al., International Conference on Learning Representations (2021)\\n119. E. Parisotto, R. Salakhutdinov, International Conference on Learning Representations\\n(2021)\\n120. S. Wu, et al., Advances in Neural Information Processing Systems 33 (2020)\\n121. A. Bosselut, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)\\n123. K. Choi, et al., International Conference on Machine Learning (2020)\\n124. C. Doersch, A. Gupta, A. Zisserman, Advances in Neural Information Processing Sys-\\ntems 33 pp. 21,981–21,993 (2020)\\n125. N. Carion, et al., European Conference on Computer Vision pp. 213—-229 (2020)\\n126. X. Zhu, et al., International Conference on Learning Representations (2021)\\n127. X. Liu, et al., International Conference on Machine Learning pp. 6327–6335 (2020)\\n128. J. Kasai, et al., International Conference on Machine Learning (2020)\\n129. D. Hudson, L. Zitnick, International Conference on Machine Learning pp. 4487–4499\\n(2021)\\n130. A. Radford, et al., Technical Report, OpenAI (2018)\\n131. A. Radford, et al., OpenAI blog p. 9 (2019)\\n132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)\\n134. Z. Dai, et al., Proceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics pp. 2978–2988 (2019)\\n135. E. Parisotto, International Conference on Machine Learning (2020)\\n136. X. Ma, et al., Advances in Neural Information Processing Systems 32 pp. 2232–2242\\n(2019)\\n137. L. Lathauwer, SIAM Journal on Matrix Analysis and Applications 30(3), 1033 (2008)\\n138. L. Tucker, Psychometrika 31(3), 279 (1966)\\n139. J. Devlin, et al., Proceedings of NAACL-HLT 2019 pp. 4171–4186 (2019)\\n140. W. Taylor, Journalism Bulletin 30(4), 415 (1953)\\n141. K. Clark, et al., arXiv preprint arXiv:1906.04341 (2019)\\n142. S. Sun, et al., Proceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)\\n143. W. Wang, et al., Advances in Neural Information Processing Systems 33 (2020)\\n144. J. McCarley, R. Chakravarti, A. Sil, arXiv preprint arXiv:1910.06360 (2020)\\n145. O. Zafrir, et al., The 5th Workshop on Energy Eﬃcient Machine Learning and Cogni-\\ntive Computing - NeurIPS (2019)\\n146. M. Joshi, et al., In Proceedings of the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing pp. 5803–5808 (2019)\\n147. L. Gong, et al., International Conference on Machine Learning pp. 2337–2346 (2019)\\n148. Z. Lan, et al., International Conference on Learning Representations (2020)\\n149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)\\n151. M. Joshi, et al., Transactions of the Association for Computational Linguistics 8, 64\\n(2020)\\n152. Y. Liu, et al., arXiv preprint arXiv:1907.11692 (2019)\\n153. P. He, et al., International Conference on Learning Representations (2021)\\n154. V. Sanh, et al., the 5th Workshop on Energy Eﬃcient Machine Learning and Cognitive\\nComputing - NeurIPS (2019)\\n22\\nDerya Soydaner\\n155. W. Wang, et al., International Conference on Learning Representations (2020)\\n156. S. Shen, et al., AAAI Conference on Artiﬁcial Intelligence 34, 8815 (2020)\\n157. J. Lee, et al., Bioinformatics 36(4), 1234 (2020)\\n158. P. Prakash, et al., AAAI Conference on Artiﬁcial Intelligence 35, 453 (2021)\\n159. Z. Wu, et al., International Conference on Learning Representations (2020)\\n160. S. Mehta, et al., International Conference on Learning Representations (2021)\\n161. Y. Tay, et al., International Conference on Learning Representations (2021)\\n162. S. Yun, et al., International Conference on Learning Representations (2018)\\n163. Y. Rong, et al., Advances in Neural Information Processing Systems 33 (2020)\\n164. J. Yang, et al., Advances in Neural Information Processing Systems 34 (2021)\\n165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)\\n167. X. Fan, et al., Advances in Neural Information Processing Systems 33 (2020)\\n168. G. Brunner, et al., International Conference on Learning Representations (2020)\\n169. A. Dosovitskiy, et al., International Conference on Learning Representations (2021)\\n170. A. Katharopoulos, et al., International Conference on Machine Learning (2020)\\n171. L. Greengard, V. Rokhlin, Journal of Computational Physics 73(2), 325– (1987)\\n172. T. Nguyen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n173. N. Kitaev, L. Kaiser, A. Levskaya, International Conference on Learning Representa-\\ntions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)\\n176. R. Child, et al., arXiv preprint arXiv:1904.10509 (2019)\\n177. G. Correia, V. Niculae, A. Martins, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing pp. 2174–2184 (2019)\\n178. H. Peng, et al., International Conference on Learning Representations (2021)\\n179. Y. Chen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n180. M. Zaheer, et al., Advances in Neural Information Processing Systems 33 (2020)\\n181. C.Z. Huang, et al., International Conference on Learning Representations (2019)\\n182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer\\nVision pp. 377–386 (2021)\\n184. C. Zhu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n185. A. Jaegle, et al., International Conference on Machine Learning pp. 4651–4664 (2021)\\n186. K. Choromanski, et al., International Conference on Learning Representations (2021)\\n187. A. El-Nouby, et al., Advances in Neural Information Processing Systems 34 (2021)\\n188. Q. Yu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n189. Z. Zeng, et al., International Conference on Machine Learning pp. 12,321–12,332 (2021)\\n190. Z. Shen, et al., Proceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision pp. 3531–3539 (2021)\\n191. S. Luo, et al., Advances in Neural Information Processing Systems 34 (2021)\\n192. X. Ma, et al., Advances in Neural Information Processing Systems 34 (2021)\\n')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WikipediaLoader(query=\"Fever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=JSONLoader(\"dummy.json\",jq_schema=\".\",text_content=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'speech.txt'}, page_content='Prime Minister Narendra Modi\\'s recent speeches emphasize India\\'s \\nfuture ambitions and developmental strategies. For instance, \\nduring the 78th Independence Day address, Modi focused on India \\nbecoming a global leader in several sectors, including renewable \\nenergy and high-tech manufacturing. He highlighted projects like \\nthe \"Green Hydrogen Mission,\" which aims to make India a \\npowerhouse in green energy, and \"Design in India, Design for the \\nWorld,\" encouraging domestic innovations with international \\nimpact. Additionally, his speech included goals to strengthen \\nIndia\\'s position in semiconductor production, enhance skill \\ntraining, and make significant advances in public health through initiatives like the Swasth Bharat Missionâ€‹')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter,CharacterTextSplitter,HTMLHeaderTextSplitter,RecursiveJsonSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc=splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"Prime Minister Narendra Modi's recent speeches emphasize India's\"),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='future ambitions and developmental strategies. For instance,'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='during the 78th Independence Day address, Modi focused on India'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='becoming a global leader in several sectors, including renewable'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='energy and high-tech manufacturing. He highlighted projects like'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='the \"Green Hydrogen Mission,\" which aims to make India a'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='powerhouse in green energy, and \"Design in India, Design for the'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='World,\" encouraging domestic innovations with international'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='impact. Additionally, his speech included goals to strengthen'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content=\"India's position in semiconductor production, enhance skill\"),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='training, and make significant advances in public health through initiatives like the Swasth Bharat'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='the Swasth Bharat Missionâ€‹')]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=CharacterTextSplitter(chunk_size=100,chunk_overlap=20,separator= \"\\n\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc=splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"Prime Minister Narendra Modi's recent speeches emphasize India's\"),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='future ambitions and developmental strategies. For instance,'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='during the 78th Independence Day address, Modi focused on India'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='becoming a global leader in several sectors, including renewable'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='energy and high-tech manufacturing. He highlighted projects like'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='the \"Green Hydrogen Mission,\" which aims to make India a'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='powerhouse in green energy, and \"Design in India, Design for the'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='World,\" encouraging domestic innovations with international'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='impact. Additionally, his speech included goals to strengthen'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content=\"India's position in semiconductor production, enhance skill\"),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='training, and make significant advances in public health through initiatives like the Swasth Bharat Missionâ€‹')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Sample Page</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 20px;\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "        }\n",
    "        p {\n",
    "            color: #555;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Header h1</h1>\n",
    "    <p>This is a simple HTML page represented as a string in Python</p>\n",
    "    <h2>Header h2</h2>\n",
    "    <p>This is a simple HTML page</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on=[\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "]\n",
    "splitter=HTMLHeaderTextSplitter(headers_to_split_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc=splitter.split_text(html_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Header h1'}, page_content='This is a simple HTML page represented as a string in Python'),\n",
       " Document(metadata={'Header 1': 'Header h1', 'Header 2': 'Header h2'}, page_content='This is a simple HTML page')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveJsonSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc=splitter.split_text(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"My name is Aman Kumar Jha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=embedder.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0056478166952729225,\n",
       " -0.02761833183467388,\n",
       " -0.016989480704069138,\n",
       " -0.03983635827898979,\n",
       " 0.048823002725839615,\n",
       " 0.0191193874925375,\n",
       " 0.0039197648875415325,\n",
       " 0.02091209776699543,\n",
       " -0.050391267985105515,\n",
       " 0.04476655274629593,\n",
       " 0.01761258766055107,\n",
       " 0.04543423652648926,\n",
       " -0.04189089685678482,\n",
       " -0.04574520140886307,\n",
       " -0.027584461495280266,\n",
       " 0.014353190548717976,\n",
       " 0.01628841832280159,\n",
       " 0.04649839922785759,\n",
       " 0.015504574403166771,\n",
       " 0.00025714249932207167,\n",
       " -0.04091449826955795,\n",
       " 0.031288035213947296,\n",
       " -0.02246871404349804,\n",
       " -0.048576291650533676,\n",
       " -0.0026034903712570667,\n",
       " 0.027444520965218544,\n",
       " 0.0009247663547284901,\n",
       " -0.020754478871822357,\n",
       " -0.09758508205413818,\n",
       " 0.09067187458276749,\n",
       " -0.0637902095913887,\n",
       " -0.00023128361499402672,\n",
       " -0.064623162150383,\n",
       " 0.04984699562191963,\n",
       " 0.011969184502959251,\n",
       " -0.059520211070775986,\n",
       " -0.02620447613298893,\n",
       " -0.03049655072391033,\n",
       " 0.05421436205506325,\n",
       " 0.030765024945139885,\n",
       " -0.048663489520549774,\n",
       " -0.05630573630332947,\n",
       " -0.008645697496831417,\n",
       " -0.04436101019382477,\n",
       " 0.02797093614935875,\n",
       " -0.016045045107603073,\n",
       " -0.009493399411439896,\n",
       " 0.017436204478144646,\n",
       " 0.039180152118206024,\n",
       " -0.028043240308761597,\n",
       " 0.006144654471427202,\n",
       " 0.008829502388834953,\n",
       " 0.025502149015665054,\n",
       " -0.03325128182768822,\n",
       " -0.0015613788273185492,\n",
       " -0.01645137183368206,\n",
       " 0.001882993965409696,\n",
       " 0.07221624255180359,\n",
       " 0.004036023747175932,\n",
       " -0.011761109344661236,\n",
       " 0.013887632638216019,\n",
       " 0.01830497942864895,\n",
       " 0.006203986704349518,\n",
       " 0.025256076827645302,\n",
       " -0.0036078249104321003,\n",
       " -0.009579729288816452,\n",
       " -0.016126461327075958,\n",
       " 0.020227422937750816,\n",
       " 0.015297150239348412,\n",
       " 0.00255526858381927,\n",
       " 0.023344354704022408,\n",
       " -0.026005182415246964,\n",
       " 0.050834108144044876,\n",
       " -0.056883085519075394,\n",
       " -0.04450714588165283,\n",
       " -0.0869636982679367,\n",
       " 0.006816281005740166,\n",
       " 0.024284934625029564,\n",
       " 0.020508555695414543,\n",
       " 0.023542633280158043,\n",
       " 0.00954616442322731,\n",
       " -0.05345899239182472,\n",
       " -0.035098157823085785,\n",
       " 0.006751423701643944,\n",
       " -0.0769641101360321,\n",
       " 0.0015572671545669436,\n",
       " -0.015216040425002575,\n",
       " 0.0153712909668684,\n",
       " -0.0085305692628026,\n",
       " -0.014916403219103813,\n",
       " -0.017022009938955307,\n",
       " -0.05304986611008644,\n",
       " -0.007009302731603384,\n",
       " -0.07607675343751907,\n",
       " -0.005166035145521164,\n",
       " 0.07987026870250702,\n",
       " -0.008949055336415768,\n",
       " -0.051689647138118744,\n",
       " 0.01657584123313427,\n",
       " -0.012949081137776375,\n",
       " 0.044801078736782074,\n",
       " -0.003594575682654977,\n",
       " -0.043253444135189056,\n",
       " -0.001928078243508935,\n",
       " 0.028410473838448524,\n",
       " 0.033138457685709,\n",
       " 0.02361193113029003,\n",
       " 0.04818427190184593,\n",
       " -0.02110493928194046,\n",
       " 0.05345657840371132,\n",
       " -0.02114678919315338,\n",
       " 0.023790482431650162,\n",
       " -0.0018295224290341139,\n",
       " -0.028359832242131233,\n",
       " 0.03313988074660301,\n",
       " -0.0043728225864470005,\n",
       " -0.007083891425281763,\n",
       " 0.04496641829609871,\n",
       " -0.0027754639741033316,\n",
       " 0.006897839717566967,\n",
       " 0.0013127934653311968,\n",
       " 0.00863255001604557,\n",
       " 0.08458040654659271,\n",
       " -0.02046719379723072,\n",
       " -0.06902358680963516,\n",
       " 0.0001028471378958784,\n",
       " 0.029398614540696144,\n",
       " 0.015382356941699982,\n",
       " 0.06387870013713837,\n",
       " 0.015538507141172886,\n",
       " -0.007088290993124247,\n",
       " -0.01599021628499031,\n",
       " 0.03932512551546097,\n",
       " -0.010195799171924591,\n",
       " 0.007310191169381142,\n",
       " 0.00893317349255085,\n",
       " -0.009723779745399952,\n",
       " -0.0127492044121027,\n",
       " 0.03526496887207031,\n",
       " -0.011929131112992764,\n",
       " -0.02478962577879429,\n",
       " 0.011248244903981686,\n",
       " 0.027892714366316795,\n",
       " -0.017303111031651497,\n",
       " 0.05626754090189934,\n",
       " 0.06657808274030685,\n",
       " -0.010271715000271797,\n",
       " -0.05557268485426903,\n",
       " 0.05953272432088852,\n",
       " 0.003446064656600356,\n",
       " 0.03610315918922424,\n",
       " -0.008884463459253311,\n",
       " -0.0648062601685524,\n",
       " -0.024387583136558533,\n",
       " 0.07621003687381744,\n",
       " 0.026903413236141205,\n",
       " -0.04048651456832886,\n",
       " -0.018117617815732956,\n",
       " -0.02550787664949894,\n",
       " -0.0022621829994022846,\n",
       " 0.03597847372293472,\n",
       " -0.004022860899567604,\n",
       " 0.08043885976076126,\n",
       " 0.0023056468926370144,\n",
       " -0.018247809261083603,\n",
       " -0.033459000289440155,\n",
       " 0.06117790564894676,\n",
       " -0.003932008519768715,\n",
       " -0.014750919304788113,\n",
       " 0.04866130277514458,\n",
       " -0.033900100737810135,\n",
       " 0.0126136289909482,\n",
       " -0.06773444265127182,\n",
       " -0.04787145182490349,\n",
       " -0.01236772071570158,\n",
       " -0.056862931698560715,\n",
       " 0.031507208943367004,\n",
       " 0.024240991100668907,\n",
       " -0.03038444183766842,\n",
       " 0.025392508134245872,\n",
       " -0.010188399814069271,\n",
       " -0.04639936238527298,\n",
       " -0.0034120238851755857,\n",
       " 0.045041728764772415,\n",
       " 0.006377387326210737,\n",
       " 0.01054160576313734,\n",
       " 0.07315835356712341,\n",
       " -0.02123340778052807,\n",
       " -0.0860803946852684,\n",
       " 0.02124764956533909,\n",
       " -0.05870884656906128,\n",
       " 0.00024935900000855327,\n",
       " 0.02560109831392765,\n",
       " -0.005584189668297768,\n",
       " -0.004311942961066961,\n",
       " 0.04278472438454628,\n",
       " -0.004394580144435167,\n",
       " 0.030247941613197327,\n",
       " -0.01890358328819275,\n",
       " 0.01822848618030548,\n",
       " -0.04820733144879341,\n",
       " 0.07455916702747345,\n",
       " 0.008000933565199375,\n",
       " 0.00453212158754468,\n",
       " 0.01759784109890461,\n",
       " -0.063047856092453,\n",
       " 0.01412968523800373,\n",
       " -0.06325595080852509,\n",
       " -0.002945717889815569,\n",
       " 0.03411206230521202,\n",
       " -0.07650846987962723,\n",
       " 0.03309224173426628,\n",
       " -0.01347037311643362,\n",
       " 0.03454500064253807,\n",
       " 0.014809778891503811,\n",
       " -0.021621661260724068,\n",
       " -0.02828296460211277,\n",
       " 0.03494657203555107,\n",
       " 0.004937591962516308,\n",
       " -0.008548406884074211,\n",
       " 0.07418056577444077,\n",
       " 0.017398906871676445,\n",
       " 0.005509108304977417,\n",
       " -0.01906931772828102,\n",
       " 0.026552714407444,\n",
       " 0.019039053469896317,\n",
       " -0.016954800114035606,\n",
       " 0.02808196097612381,\n",
       " -0.02301386557519436,\n",
       " -0.028572265058755875,\n",
       " -0.033065225929021835,\n",
       " 0.07105938345193863,\n",
       " -0.03453363478183746,\n",
       " -0.03680256009101868,\n",
       " 0.03116018883883953,\n",
       " -0.024952849373221397,\n",
       " 0.021853215992450714,\n",
       " 0.021801603958010674,\n",
       " 0.027454422786831856,\n",
       " 0.03117947094142437,\n",
       " -0.07158900797367096,\n",
       " -0.02590995840728283,\n",
       " 0.05177323520183563,\n",
       " 0.038631871342659,\n",
       " -0.019188350066542625,\n",
       " -0.07000178843736649,\n",
       " -0.04473255202174187,\n",
       " 0.037198469042778015,\n",
       " 0.014913186430931091,\n",
       " 0.04656834900379181,\n",
       " -0.0014022213872522116,\n",
       " -0.0987185686826706,\n",
       " 0.04462316632270813,\n",
       " -0.0040772538632154465,\n",
       " -0.0398772768676281,\n",
       " 0.06323122978210449,\n",
       " -0.04750622808933258,\n",
       " -0.03173065558075905,\n",
       " -0.028007391840219498,\n",
       " 0.015805460512638092,\n",
       " 0.06458808481693268,\n",
       " -0.021942952647805214,\n",
       " 0.017072811722755432,\n",
       " 0.020549552515149117,\n",
       " -0.025417864322662354,\n",
       " 0.00404691556468606,\n",
       " 0.017605997622013092,\n",
       " -0.045379675924777985,\n",
       " -0.008089595474302769,\n",
       " -0.03180660679936409,\n",
       " 0.031332891434431076,\n",
       " -0.06778395920991898,\n",
       " 0.0312732569873333,\n",
       " -0.026437925174832344,\n",
       " -0.02372691035270691,\n",
       " 0.0011889331508427858,\n",
       " -0.021616172045469284,\n",
       " 0.03729471191763878,\n",
       " -0.005024609621614218,\n",
       " -0.042372848838567734,\n",
       " 0.022741975262761116,\n",
       " 0.015153145417571068,\n",
       " -0.01794956438243389,\n",
       " -0.03235308825969696,\n",
       " -0.03690981864929199,\n",
       " -0.012743303552269936,\n",
       " -0.024786055088043213,\n",
       " -0.020938139408826828,\n",
       " 0.028652384877204895,\n",
       " -0.06581390649080276,\n",
       " -0.03281465545296669,\n",
       " 0.010737692005932331,\n",
       " 0.0041931066662073135,\n",
       " -0.030710875988006592,\n",
       " -0.0383295863866806,\n",
       " 0.02598034031689167,\n",
       " -0.008338632062077522,\n",
       " 0.009779318235814571,\n",
       " 0.023409249261021614,\n",
       " 0.011029188521206379,\n",
       " 0.010567919351160526,\n",
       " 0.007876323536038399,\n",
       " 0.0384407639503479,\n",
       " -0.05340036377310753,\n",
       " 0.014067497104406357,\n",
       " -0.04378296062350273,\n",
       " -0.013622496277093887,\n",
       " -0.022701779380440712,\n",
       " 0.009438593871891499,\n",
       " -0.00869153905659914,\n",
       " -0.016394034028053284,\n",
       " -0.07358749210834503,\n",
       " -0.03622742369771004,\n",
       " 0.011249870993196964,\n",
       " 0.006268571596592665,\n",
       " 0.020345091819763184,\n",
       " -0.03189883381128311,\n",
       " 0.0470668226480484,\n",
       " -0.04836936295032501,\n",
       " 0.03996531292796135,\n",
       " -0.005614629480987787,\n",
       " 0.0991387814283371,\n",
       " -0.0006128354580141604,\n",
       " 0.0468238927423954,\n",
       " -0.02649141475558281,\n",
       " 0.034539300948381424,\n",
       " -0.02966272085905075,\n",
       " 0.05816054344177246,\n",
       " -0.007091055158525705,\n",
       " -0.015938419848680496,\n",
       " -0.009966800920665264,\n",
       " 0.037625785917043686,\n",
       " 0.0012525670463219285,\n",
       " 0.03654740750789642,\n",
       " 0.008754206821322441,\n",
       " 0.03683251142501831,\n",
       " -0.03554694354534149,\n",
       " 0.0018084037583321333,\n",
       " -0.05328354984521866,\n",
       " -0.010275253094732761,\n",
       " 0.010257896035909653,\n",
       " 0.020074110478162766,\n",
       " -0.042300887405872345,\n",
       " -0.04453122243285179,\n",
       " -0.041338395327329636,\n",
       " -0.008106541819870472,\n",
       " 0.0069066914729774,\n",
       " 0.03832410275936127,\n",
       " 0.06010628864169121,\n",
       " -0.04104280844330788,\n",
       " 0.03136463463306427,\n",
       " 0.07405343651771545,\n",
       " -0.055321525782346725,\n",
       " -0.02291625365614891,\n",
       " -0.015180240385234356,\n",
       " -0.025439463555812836,\n",
       " 0.04904986917972565,\n",
       " 0.00995652750134468,\n",
       " -0.028008315712213516,\n",
       " -0.029926564544439316,\n",
       " -0.009492607787251472,\n",
       " 0.05442310869693756,\n",
       " 0.0218073558062315,\n",
       " -0.004428706597536802,\n",
       " -0.03221185877919197,\n",
       " 0.04400904104113579,\n",
       " -0.013030794449150562,\n",
       " 0.03378177061676979,\n",
       " 0.011524873785674572,\n",
       " 0.03335444629192352,\n",
       " 0.014626938849687576,\n",
       " -0.0180441215634346,\n",
       " -0.015064512379467487,\n",
       " -0.015563422814011574,\n",
       " 0.051173996180295944,\n",
       " -0.05938355252146721,\n",
       " -0.07013094425201416,\n",
       " 0.002890932373702526,\n",
       " 0.031160829588770866,\n",
       " -0.03972644731402397,\n",
       " -0.00803754385560751,\n",
       " -0.013170897029340267,\n",
       " 0.052961453795433044,\n",
       " -0.02142731286585331,\n",
       " 0.003919337410479784,\n",
       " -0.00536110857501626,\n",
       " -0.01286927331238985,\n",
       " 0.05215765908360481,\n",
       " 0.018125776201486588,\n",
       " 0.05609530955553055,\n",
       " 0.06649231910705566,\n",
       " 0.02160300873219967,\n",
       " 0.0451679453253746,\n",
       " 0.032702695578336716,\n",
       " 0.0016467297682538629,\n",
       " -0.07109788805246353,\n",
       " -0.011392156593501568,\n",
       " -0.07375301420688629,\n",
       " 0.010356049984693527,\n",
       " -0.005783412605524063,\n",
       " 0.010412891395390034,\n",
       " -0.03548074886202812,\n",
       " -0.00956098735332489,\n",
       " -0.06412115693092346,\n",
       " -0.0021386758890002966,\n",
       " -0.01674276404082775,\n",
       " 0.003229157067835331,\n",
       " 0.011237306520342827,\n",
       " -0.013486838899552822,\n",
       " -0.02622382901608944,\n",
       " 0.01524888351559639,\n",
       " 0.04944510757923126,\n",
       " 0.04578065127134323,\n",
       " -0.05201949179172516,\n",
       " -0.08604583889245987,\n",
       " -0.017813825979828835,\n",
       " 0.08513782918453217,\n",
       " -0.025941696017980576,\n",
       " -0.028975576162338257,\n",
       " 0.024025503545999527,\n",
       " 0.018506625667214394,\n",
       " -0.004380481317639351,\n",
       " -0.017341837286949158,\n",
       " -0.044514965265989304,\n",
       " -0.04806212708353996,\n",
       " -0.025366555899381638,\n",
       " -0.003060805145651102,\n",
       " -0.028397588059306145,\n",
       " 0.004397897981107235,\n",
       " 0.03057059459388256,\n",
       " 0.009681959636509418,\n",
       " 0.021215280517935753,\n",
       " -0.011775299906730652,\n",
       " 0.006216953042894602,\n",
       " -0.04731329530477524,\n",
       " -0.06956377625465393,\n",
       " 0.018129341304302216,\n",
       " 0.015912527218461037,\n",
       " -0.013615040108561516,\n",
       " -0.03630240261554718,\n",
       " 0.06960427761077881,\n",
       " 0.0016670062905177474,\n",
       " 0.014248259365558624,\n",
       " 0.014407756738364697,\n",
       " -0.04930106922984123,\n",
       " -0.04924728348851204,\n",
       " -0.0006929157534614205,\n",
       " -0.012759462930262089,\n",
       " 0.002601016778498888,\n",
       " -0.10579003393650055,\n",
       " 0.06124149262905121,\n",
       " -0.052953846752643585,\n",
       " -0.011650962755084038,\n",
       " -0.07776055485010147,\n",
       " -0.03519248589873314,\n",
       " -0.0089436499401927,\n",
       " -0.018185168504714966,\n",
       " 0.08807463943958282,\n",
       " -0.009679842740297318,\n",
       " -0.0011249171802774072,\n",
       " -0.033888738602399826,\n",
       " -0.04317217320203781,\n",
       " 0.022516503930091858,\n",
       " -0.0504499226808548,\n",
       " -0.012899836525321007,\n",
       " -0.033140502870082855,\n",
       " 0.0010102945379912853,\n",
       " -0.007781635504215956,\n",
       " 0.07119570672512054,\n",
       " 0.035973887890577316,\n",
       " 0.025815648958086967,\n",
       " -0.05115705356001854,\n",
       " 0.001981940120458603,\n",
       " 0.023320814594626427,\n",
       " -0.03826116397976875,\n",
       " 0.027041474357247353,\n",
       " -0.09011982381343842,\n",
       " 0.06531929224729538,\n",
       " 0.00135071761906147,\n",
       " -0.023028619587421417,\n",
       " -0.023319536820054054,\n",
       " 0.002304217079654336,\n",
       " 0.04109443724155426,\n",
       " 0.015822596848011017,\n",
       " -0.0038812209386378527,\n",
       " 0.01335363183170557,\n",
       " 0.0005589952343143523,\n",
       " -0.0038373423740267754,\n",
       " -0.00813203677535057,\n",
       " 0.042941268533468246,\n",
       " -0.01162409596145153,\n",
       " 0.03217969462275505,\n",
       " -0.012624596245586872,\n",
       " 0.004908525850623846,\n",
       " 0.014352490194141865,\n",
       " 0.01345501746982336,\n",
       " 0.011811960488557816,\n",
       " -0.026224210858345032,\n",
       " 0.025042962282896042,\n",
       " 0.006012726575136185,\n",
       " -0.018302707001566887,\n",
       " -0.011248352937400341,\n",
       " 0.002091408707201481,\n",
       " -0.008116582408547401,\n",
       " 0.06966905295848846,\n",
       " -0.04771186411380768,\n",
       " 0.019914835691452026,\n",
       " -0.01872378960251808,\n",
       " -0.018513953313231468,\n",
       " -0.004946424625813961,\n",
       " 0.0012238096678629518,\n",
       " 0.0880015566945076,\n",
       " -0.00821688026189804,\n",
       " -0.04649702087044716,\n",
       " 0.06599418073892593,\n",
       " 0.010037754662334919,\n",
       " 0.009255660697817802,\n",
       " -0.009730663150548935,\n",
       " 0.025560254231095314,\n",
       " 0.03084256313741207,\n",
       " 0.043951913714408875,\n",
       " -0.025981858372688293,\n",
       " -0.06006835028529167,\n",
       " -0.015753459185361862,\n",
       " 0.0696723610162735,\n",
       " -0.05274531990289688,\n",
       " 0.04424198344349861,\n",
       " 0.011398563161492348,\n",
       " 0.009284530766308308,\n",
       " -0.005416973028331995,\n",
       " -0.0383593775331974,\n",
       " 0.08289165794849396,\n",
       " -0.08387259393930435,\n",
       " 0.01776869036257267,\n",
       " 0.012732794508337975,\n",
       " 0.0009576529264450073,\n",
       " 0.009558158926665783,\n",
       " -0.008471139706671238,\n",
       " -0.016416210681200027,\n",
       " -0.04442388191819191,\n",
       " 0.022143786773085594,\n",
       " -0.02488834597170353,\n",
       " 0.008266163989901543,\n",
       " 0.008806457743048668,\n",
       " 0.006486993748694658,\n",
       " 0.01547173596918583,\n",
       " -0.029788754880428314,\n",
       " -0.1091800183057785,\n",
       " 0.03394414484500885,\n",
       " -0.007259609177708626,\n",
       " 0.04626752808690071,\n",
       " 0.00944947823882103,\n",
       " 0.06982801109552383,\n",
       " -0.024696774780750275,\n",
       " 0.003688025986775756,\n",
       " -0.013418261893093586,\n",
       " -0.03146866336464882,\n",
       " -0.047293271869421005,\n",
       " 0.0038460385985672474,\n",
       " 0.007416611537337303,\n",
       " -0.0014610190410166979,\n",
       " 0.00773973111063242,\n",
       " 0.00941669661551714,\n",
       " -0.026105573400855064,\n",
       " 0.06622727960348129,\n",
       " -0.006503878626972437,\n",
       " -0.014293204993009567,\n",
       " -0.0017301823245361447,\n",
       " 0.04075327515602112,\n",
       " -0.06700101494789124,\n",
       " -0.0025251300539821386,\n",
       " 0.020000623539090157,\n",
       " 0.03061666339635849,\n",
       " -0.03976357728242874,\n",
       " 0.04988778755068779,\n",
       " -0.0437258705496788,\n",
       " -0.026164157316088676,\n",
       " 0.00012629556294996291,\n",
       " 0.03371799737215042,\n",
       " 0.02365555427968502,\n",
       " 0.052973050624132156,\n",
       " 0.011737756431102753,\n",
       " -0.028583725914359093,\n",
       " 0.05572601407766342,\n",
       " 0.013240632601082325,\n",
       " 0.02986966073513031,\n",
       " 0.011487754061818123,\n",
       " 0.017081448808312416,\n",
       " 0.06933173537254333,\n",
       " 0.0356609933078289,\n",
       " -0.025692272931337357,\n",
       " 0.04202134534716606,\n",
       " -0.06124361604452133,\n",
       " 0.04395145922899246,\n",
       " -0.028287915512919426,\n",
       " 0.013903778977692127,\n",
       " -0.025791827589273453,\n",
       " -0.00021781657414976507,\n",
       " 0.026861514896154404,\n",
       " 0.030861055478453636,\n",
       " -0.03767542541027069,\n",
       " -0.037586066871881485,\n",
       " 0.029577352106571198,\n",
       " -0.03704635053873062,\n",
       " 0.08263334631919861,\n",
       " -0.026454590260982513,\n",
       " 0.004455309361219406,\n",
       " -0.011763054877519608,\n",
       " 0.011058567091822624,\n",
       " 0.023674972355365753,\n",
       " 0.010766633786261082,\n",
       " -0.03521767631173134,\n",
       " 0.01718548685312271,\n",
       " -0.049994539469480515,\n",
       " -0.023804647848010063,\n",
       " 0.010213684290647507,\n",
       " 0.012928647920489311,\n",
       " 0.011081462725996971,\n",
       " -0.028985148295760155,\n",
       " -0.013458876870572567,\n",
       " -0.02714892104268074,\n",
       " -0.03112386353313923,\n",
       " -0.034330226480960846,\n",
       " -0.012365898117423058,\n",
       " 0.053816281259059906,\n",
       " -0.009660501033067703,\n",
       " -0.0140154380351305,\n",
       " 0.03159545361995697,\n",
       " 0.056484535336494446,\n",
       " 0.012069438584148884,\n",
       " 0.013894302770495415,\n",
       " 0.039057422429323196,\n",
       " 0.005712865851819515,\n",
       " -0.00025852996623143554,\n",
       " -0.03009205311536789,\n",
       " 0.015917489305138588,\n",
       " 0.03380586579442024,\n",
       " -0.027724994346499443,\n",
       " -0.0206061452627182,\n",
       " 0.008616013452410698,\n",
       " -0.07781706005334854,\n",
       " 0.04440141096711159,\n",
       " 0.0276651531457901,\n",
       " -0.01532784290611744,\n",
       " -0.02661258913576603,\n",
       " 0.017610687762498856,\n",
       " -0.02894585207104683,\n",
       " -0.06443258374929428,\n",
       " -0.027062518522143364,\n",
       " 0.01587482914328575,\n",
       " -0.022181635722517967,\n",
       " -0.01127498596906662,\n",
       " 0.05629684031009674,\n",
       " 0.02399342507123947,\n",
       " 0.024633901193737984,\n",
       " 0.02296987548470497,\n",
       " -0.03402097895741463,\n",
       " -0.01998324505984783,\n",
       " 0.04183370620012283,\n",
       " -0.003456810722127557,\n",
       " -0.025028664618730545,\n",
       " 0.034649983048439026,\n",
       " -0.00287296948954463,\n",
       " -0.01756533980369568,\n",
       " 0.038156330585479736,\n",
       " -0.023045629262924194,\n",
       " -0.05533305183053017,\n",
       " -0.04859236255288124,\n",
       " -0.017271872609853745,\n",
       " 0.005175010766834021,\n",
       " -0.0624488890171051,\n",
       " -0.008754952810704708,\n",
       " 0.050589680671691895,\n",
       " -0.0358089841902256,\n",
       " 0.04021323844790459,\n",
       " 0.016003349795937538,\n",
       " -0.02764420211315155,\n",
       " 0.002470369217917323,\n",
       " 0.006414434872567654,\n",
       " 0.014877564273774624,\n",
       " -0.004762019496411085,\n",
       " -0.036305706948041916,\n",
       " 2.4387671146541834e-05,\n",
       " 0.007710250560194254,\n",
       " 0.026708317920565605,\n",
       " 0.029688743874430656,\n",
       " 0.016138168051838875,\n",
       " -0.006735263857990503,\n",
       " -0.046419769525527954,\n",
       " -0.0012020705034956336,\n",
       " 0.028565766289830208,\n",
       " -0.002922329120337963,\n",
       " -0.028668534010648727,\n",
       " 0.024871444329619408,\n",
       " 0.12089817970991135,\n",
       " -0.022086506709456444,\n",
       " -0.00742438156157732,\n",
       " 8.610403165221214e-05,\n",
       " -0.038361359387636185,\n",
       " 0.0731038749217987,\n",
       " -0.04987958073616028,\n",
       " 0.02156204730272293,\n",
       " 0.035785868763923645,\n",
       " 0.051166385412216187,\n",
       " 0.027172500267624855,\n",
       " 0.060266125947237015,\n",
       " 0.006474605295807123,\n",
       " -0.03667809069156647,\n",
       " 0.04331720620393753,\n",
       " 0.03100678324699402,\n",
       " 0.03775738179683685,\n",
       " -0.02840685471892357,\n",
       " -0.03358845040202141,\n",
       " 0.03191335126757622,\n",
       " 0.0488240122795105,\n",
       " -0.009555960074067116,\n",
       " 0.026166897267103195,\n",
       " -0.035394757986068726,\n",
       " -0.04812020808458328,\n",
       " 0.030361657962203026,\n",
       " 0.06181180849671364,\n",
       " -0.008384169079363346,\n",
       " 0.06127655878663063,\n",
       " -0.006923208478838205,\n",
       " -0.036610811948776245,\n",
       " 0.05472108721733093,\n",
       " 0.004507387988269329,\n",
       " -0.00627813721075654,\n",
       " -0.042332910001277924,\n",
       " -0.021770592778921127,\n",
       " 0.12896592915058136,\n",
       " -0.021381158381700516,\n",
       " -0.03014945238828659,\n",
       " 0.06385551393032074,\n",
       " -0.03563263639807701,\n",
       " 0.034257110208272934,\n",
       " -0.025883903726935387,\n",
       " 0.015526427887380123,\n",
       " -0.022075580433011055,\n",
       " -0.0744609534740448,\n",
       " -0.051060546189546585,\n",
       " -0.008599397726356983,\n",
       " 0.017065878957509995,\n",
       " 0.029166990891098976,\n",
       " 0.04233871400356293,\n",
       " -0.07706543058156967,\n",
       " 0.02074938826262951,\n",
       " 0.0029720852617174387,\n",
       " -0.03147343918681145,\n",
       " -0.0029807656537741423,\n",
       " -0.005080340895801783,\n",
       " 0.012084802612662315,\n",
       " -0.014994907192885876,\n",
       " 0.016663696616888046,\n",
       " 0.05309772491455078,\n",
       " -0.04013151302933693,\n",
       " 0.009864579886198044,\n",
       " 0.012833918444812298,\n",
       " 0.002787622157484293,\n",
       " 0.011558320373296738,\n",
       " -0.013387482613325119,\n",
       " 0.0582459382712841,\n",
       " 0.04664558544754982,\n",
       " -0.004325307440012693,\n",
       " 0.024506963789463043,\n",
       " -0.0021237863693386316,\n",
       " -0.05596772953867912,\n",
       " 0.05058926343917847]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vatsa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb6675deb3c4c6d98800e55d95f0d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fcc7e280a94891b52eadee726544be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc76dcdb9834cb38b5ef32a01057072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effb3639a0424c5696e7350dc220871f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf503ad9058643e3aa5c0afa13277f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a18d0ca7c6487ea3e4ad4473c9f1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aebe6e05f8e4b80abea70f7d5c8b504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252c24d0196e4d0a9ee13f4b5acb0a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7672fda780ff45e69dd3f09c41576a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca77e027c384f34afcfa23c5b643cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d335543bba24511a696d605cf640011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedder=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=embedder.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.08642745018005371,\n",
       " -0.023879006505012512,\n",
       " -0.02052612602710724,\n",
       " 0.02918088622391224,\n",
       " 0.0064829373732209206,\n",
       " -0.022161560133099556,\n",
       " 0.15029487013816833,\n",
       " 0.03422492742538452,\n",
       " 0.045300427824258804,\n",
       " -0.013381884433329105,\n",
       " -0.011146591044962406,\n",
       " -0.11401952058076859,\n",
       " 0.0546744130551815,\n",
       " -0.01440649013966322,\n",
       " -0.004052078351378441,\n",
       " -0.017333094030618668,\n",
       " -0.011501909233629704,\n",
       " 0.03494712710380554,\n",
       " -0.03604423254728317,\n",
       " -0.1429554522037506,\n",
       " -0.07276467978954315,\n",
       " 0.07226624339818954,\n",
       " -0.0590011328458786,\n",
       " -0.102425217628479,\n",
       " -0.09674787521362305,\n",
       " -0.01773545704782009,\n",
       " 0.0416727140545845,\n",
       " 0.025325309485197067,\n",
       " -0.015644852072000504,\n",
       " -0.03731028363108635,\n",
       " 0.013245249167084694,\n",
       " 0.040044765919446945,\n",
       " 0.06230469048023224,\n",
       " 0.03275828808546066,\n",
       " -0.014823063276708126,\n",
       " -0.007956922985613346,\n",
       " -0.09360607713460922,\n",
       " 0.03990807756781578,\n",
       " 0.050837840884923935,\n",
       " -0.04460083693265915,\n",
       " 0.0013553393073379993,\n",
       " -0.031921613961458206,\n",
       " 0.07285454124212265,\n",
       " -0.089259572327137,\n",
       " 0.06635785847902298,\n",
       " -0.04026449844241142,\n",
       " 0.016730275005102158,\n",
       " 0.004958030767738819,\n",
       " 0.06301940232515335,\n",
       " 0.08828659355640411,\n",
       " -0.15203803777694702,\n",
       " -0.010294194333255291,\n",
       " -0.024423636496067047,\n",
       " 0.02624649368226528,\n",
       " 0.08879513293504715,\n",
       " -0.07172076404094696,\n",
       " -0.04728448763489723,\n",
       " 0.009477284736931324,\n",
       " -0.0026212597731500864,\n",
       " 0.059241265058517456,\n",
       " -0.029371516779065132,\n",
       " 0.06242251396179199,\n",
       " -0.0590091198682785,\n",
       " -0.04300177842378616,\n",
       " 0.0481354221701622,\n",
       " -0.030378321185708046,\n",
       " -0.024255504831671715,\n",
       " 0.003247381653636694,\n",
       " -0.006851689424365759,\n",
       " -0.005043262615799904,\n",
       " -0.004196295514702797,\n",
       " 0.041738636791706085,\n",
       " 0.004737454000860453,\n",
       " -0.014089246280491352,\n",
       " -0.05747710168361664,\n",
       " 0.0023537902161478996,\n",
       " 0.014298168942332268,\n",
       " 0.020595017820596695,\n",
       " 0.025160903111100197,\n",
       " 0.10853966325521469,\n",
       " -0.05515120550990105,\n",
       " -0.03124505653977394,\n",
       " -0.0028419524896889925,\n",
       " -0.04325180500745773,\n",
       " -0.07181809097528458,\n",
       " -0.005580842029303312,\n",
       " 0.0057492731139063835,\n",
       " -0.010532957501709461,\n",
       " -0.08810824155807495,\n",
       " -0.018653113394975662,\n",
       " 0.06123378127813339,\n",
       " -0.013568917289376259,\n",
       " -0.044410090893507004,\n",
       " -0.004584337119013071,\n",
       " 0.02341720089316368,\n",
       " -0.055722594261169434,\n",
       " -0.018792465329170227,\n",
       " 0.017184818163514137,\n",
       " -0.08586710691452026,\n",
       " 0.1250973641872406,\n",
       " -0.05999832600355148,\n",
       " 0.028268299996852875,\n",
       " 0.020879805088043213,\n",
       " 0.08062339574098587,\n",
       " -0.08551295846700668,\n",
       " 0.053714629262685776,\n",
       " 0.015294949524104595,\n",
       " 0.016714325174689293,\n",
       " 0.05019139125943184,\n",
       " -0.04909796267747879,\n",
       " -0.0325654037296772,\n",
       " 0.01750580035150051,\n",
       " -0.061166029423475266,\n",
       " 0.011878421530127525,\n",
       " 0.05549153313040733,\n",
       " 0.0336238369345665,\n",
       " 0.0568007156252861,\n",
       " 0.030502839013934135,\n",
       " -0.00911630503833294,\n",
       " -0.04930240660905838,\n",
       " 0.017387347295880318,\n",
       " 0.05637244135141373,\n",
       " -0.06301940232515335,\n",
       " 0.01448036264628172,\n",
       " -0.04345861077308655,\n",
       " -0.060372307896614075,\n",
       " 0.010951239615678787,\n",
       " 4.877095755847939e-34,\n",
       " -0.041781023144721985,\n",
       " -0.011830014176666737,\n",
       " 0.09007000923156738,\n",
       " 0.009088708087801933,\n",
       " -0.039929259568452835,\n",
       " -0.05169133469462395,\n",
       " -0.02836642600595951,\n",
       " -0.01571662910282612,\n",
       " 0.011191868223249912,\n",
       " -0.0474015437066555,\n",
       " 0.030848966911435127,\n",
       " 0.003058214671909809,\n",
       " 0.01165770087391138,\n",
       " -0.06353027373552322,\n",
       " -0.03545353189110756,\n",
       " 0.03413502871990204,\n",
       " 0.05640217289328575,\n",
       " -0.04058589041233063,\n",
       " -0.012082395143806934,\n",
       " 0.06767486035823822,\n",
       " 0.0002683450875338167,\n",
       " -0.07723726332187653,\n",
       " 0.034520987421274185,\n",
       " 0.0016810245579108596,\n",
       " -0.031978901475667953,\n",
       " -0.018773673102259636,\n",
       " 0.08913496881723404,\n",
       " -0.08020353317260742,\n",
       " 0.06662430614233017,\n",
       " 0.038948215544223785,\n",
       " 0.04621711000800133,\n",
       " 0.05128953605890274,\n",
       " -0.01387843955308199,\n",
       " -0.0075841485522687435,\n",
       " 0.00393677456304431,\n",
       " -0.016907429322600365,\n",
       " -0.017223114147782326,\n",
       " -0.05200999975204468,\n",
       " -0.0007370638195425272,\n",
       " 0.051744021475315094,\n",
       " 0.06463015824556351,\n",
       " 0.008831541985273361,\n",
       " -0.03408058360219002,\n",
       " -0.042157839983701706,\n",
       " -0.02661195769906044,\n",
       " -0.0032882727682590485,\n",
       " 0.06222207844257355,\n",
       " 0.0026841650251299143,\n",
       " -0.03724528104066849,\n",
       " 0.015587270259857178,\n",
       " -0.07934755831956863,\n",
       " -0.013931801542639732,\n",
       " -0.09022828936576843,\n",
       " -0.03675466775894165,\n",
       " 0.01789983920753002,\n",
       " 0.0011603791499510407,\n",
       " 0.05141317844390869,\n",
       " 0.011116261593997478,\n",
       " -0.07221538573503494,\n",
       " 0.020289316773414612,\n",
       " -0.03147664666175842,\n",
       " -0.10838721692562103,\n",
       " -0.1137344092130661,\n",
       " 0.10132645070552826,\n",
       " -0.05485636740922928,\n",
       " -0.007344176061451435,\n",
       " 0.05649508535861969,\n",
       " -0.0682259351015091,\n",
       " 0.03126130253076553,\n",
       " -0.02947160229086876,\n",
       " -0.012578117661178112,\n",
       " -0.048516061156988144,\n",
       " 0.0063510676845908165,\n",
       " 0.07597371935844421,\n",
       " -0.02020903304219246,\n",
       " -0.024706101045012474,\n",
       " -0.022747961804270744,\n",
       " 0.06880275160074234,\n",
       " -0.01716610975563526,\n",
       " 0.05123385787010193,\n",
       " 0.006112179718911648,\n",
       " 0.0954051986336708,\n",
       " -0.026534048840403557,\n",
       " -0.05527564510703087,\n",
       " 0.062072064727544785,\n",
       " -0.010955940000712872,\n",
       " -0.0032121618278324604,\n",
       " -0.05594353750348091,\n",
       " -0.03958394005894661,\n",
       " -0.0557200126349926,\n",
       " -0.053923990577459335,\n",
       " 0.029679393395781517,\n",
       " 0.06478188931941986,\n",
       " 0.01667613536119461,\n",
       " -0.04297634959220886,\n",
       " -2.815283623622439e-33,\n",
       " 0.025868412107229233,\n",
       " -0.007648027036339045,\n",
       " -0.015640463680028915,\n",
       " 0.008484958671033382,\n",
       " 0.025654636323451996,\n",
       " 0.01018103864043951,\n",
       " 0.06375057995319366,\n",
       " 0.14899559319019318,\n",
       " 0.028449922800064087,\n",
       " 0.017971975728869438,\n",
       " -0.02849867008626461,\n",
       " 0.038543131202459335,\n",
       " 0.03685113787651062,\n",
       " 0.00014556522364728153,\n",
       " 0.07631517201662064,\n",
       " -0.0025075001176446676,\n",
       " 0.012555249966681004,\n",
       " 0.04182853177189827,\n",
       " -0.02589179202914238,\n",
       " -0.014241433702409267,\n",
       " -0.08472926914691925,\n",
       " 0.10927485674619675,\n",
       " -0.05953085422515869,\n",
       " -0.03598489612340927,\n",
       " -0.004126197192817926,\n",
       " -0.01583220064640045,\n",
       " -0.01813649944961071,\n",
       " 0.08875443786382675,\n",
       " -0.003276815405115485,\n",
       " 0.06517572700977325,\n",
       " -0.032192815095186234,\n",
       " -0.04949112981557846,\n",
       " -0.19061651825904846,\n",
       " -0.002620187820866704,\n",
       " -0.04083157330751419,\n",
       " -0.030742071568965912,\n",
       " 0.019924383610486984,\n",
       " -0.06710541993379593,\n",
       " -0.022373128682374954,\n",
       " 0.03002130798995495,\n",
       " -0.056776728481054306,\n",
       " 0.0758618712425232,\n",
       " 0.013231695629656315,\n",
       " 0.0036239116452634335,\n",
       " -0.04493468627333641,\n",
       " -0.07766593247652054,\n",
       " -0.013680587522685528,\n",
       " 0.04885786026716232,\n",
       " -0.005337551236152649,\n",
       " -0.044802967458963394,\n",
       " -0.010389191098511219,\n",
       " -0.0023753924760967493,\n",
       " 0.030590936541557312,\n",
       " -0.06068522855639458,\n",
       " 0.03550022840499878,\n",
       " 0.04524526745080948,\n",
       " 0.10213441401720047,\n",
       " 0.050064846873283386,\n",
       " -0.009456801228225231,\n",
       " 0.032601017504930496,\n",
       " -0.034817084670066833,\n",
       " 0.005091619677841663,\n",
       " 0.0706106647849083,\n",
       " 0.07872486114501953,\n",
       " 0.03379314765334129,\n",
       " -0.02947278693318367,\n",
       " 0.05841666832566261,\n",
       " -0.026319852098822594,\n",
       " -0.03469746932387352,\n",
       " -0.08961159735918045,\n",
       " 0.044957712292671204,\n",
       " -0.015379541553556919,\n",
       " -0.04716048762202263,\n",
       " 0.05031257122755051,\n",
       " -0.02034701779484749,\n",
       " -0.03562958166003227,\n",
       " -0.03407349810004234,\n",
       " 0.0672723799943924,\n",
       " -0.006743198726326227,\n",
       " 0.011803656816482544,\n",
       " 0.05824533477425575,\n",
       " 0.04609955474734306,\n",
       " -0.03373461216688156,\n",
       " -0.01421579159796238,\n",
       " -0.010458840988576412,\n",
       " 0.08845990151166916,\n",
       " 0.1474248170852661,\n",
       " 0.02818155288696289,\n",
       " 0.03774220123887062,\n",
       " -0.06265575438737869,\n",
       " 0.012641699984669685,\n",
       " 0.07215768098831177,\n",
       " 0.02197692170739174,\n",
       " -0.0513029545545578,\n",
       " -0.038192152976989746,\n",
       " -2.037132951215881e-08,\n",
       " -0.005873780231922865,\n",
       " -0.055121470242738724,\n",
       " 0.06078476458787918,\n",
       " -0.007667480036616325,\n",
       " -0.05795200541615486,\n",
       " 0.025582466274499893,\n",
       " -0.00719405896961689,\n",
       " 0.043333712965250015,\n",
       " 0.0024965698830783367,\n",
       " 0.018734030425548553,\n",
       " 0.04469245672225952,\n",
       " 0.029214058071374893,\n",
       " 0.025288131088018417,\n",
       " 0.029278043657541275,\n",
       " 0.03476296365261078,\n",
       " -0.10269831866025925,\n",
       " 0.05346200615167618,\n",
       " 0.03918304294347763,\n",
       " -0.01190390344709158,\n",
       " -0.011811884120106697,\n",
       " 0.015852175652980804,\n",
       " 0.00979282520711422,\n",
       " 0.05151735618710518,\n",
       " -0.0723605677485466,\n",
       " 0.03345143422484398,\n",
       " -0.005166599992662668,\n",
       " 0.06593295931816101,\n",
       " -0.016319597139954567,\n",
       " -0.07093916833400726,\n",
       " 0.07233266532421112,\n",
       " -0.022932514548301697,\n",
       " 0.10037292540073395,\n",
       " 0.06071183830499649,\n",
       " -0.03431713208556175,\n",
       " -0.07170214504003525,\n",
       " -0.017887210473418236,\n",
       " 0.05763252452015877,\n",
       " -0.017581017687916756,\n",
       " 0.009993426501750946,\n",
       " -0.007855316624045372,\n",
       " -0.00038365699583664536,\n",
       " 0.06035952270030975,\n",
       " 0.06696031987667084,\n",
       " 0.018157191574573517,\n",
       " -0.06478334218263626,\n",
       " 0.07211234420537949,\n",
       " 0.03467920422554016,\n",
       " -0.09099839627742767,\n",
       " -0.04517121985554695,\n",
       " -0.05991576611995697,\n",
       " -0.05625051259994507,\n",
       " -0.027226587757468224,\n",
       " 0.06518712639808655,\n",
       " 0.009975743480026722,\n",
       " 0.027185093611478806,\n",
       " -0.014811648987233639,\n",
       " -0.023498373106122017,\n",
       " 0.05995374172925949,\n",
       " 0.03268793225288391,\n",
       " -0.0031355477403849363,\n",
       " 0.14355626702308655,\n",
       " 0.020503107458353043,\n",
       " -0.11630110442638397,\n",
       " -0.05170711502432823]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing vector data in Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma,FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=Chroma.from_documents(final_doc,embedder,persist_directory=\"croma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x16f55c0afd0>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=FAISS.from_documents(final_doc,embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x16f5190e050>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"db_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_db=FAISS.load_local(\"db_faiss\",embedder,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_doc = db.similarity_search(\"Who is a prime minster of india\",k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"Prime Minister Narendra Modi's recent speeches emphasize India's\")]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"Prime Minister Narendra Modi's recent speeches emphasize India's\"),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='during the 78th Independence Day address, Modi focused on India'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content=\"India's position in semiconductor production, enhance skill\"),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='becoming a global leader in several sectors, including renewable')]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Who is a prime minster of dndia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
